{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge \n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessors import Preprocessor\n",
    "from dataset import make_data_generator\n",
    "from transformer_nb2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data-giga/vocab.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-339e306e8bf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mnum_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtask_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"giga\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mVOC_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data-giga/vocab.json'"
     ]
    }
   ],
   "source": [
    "task_name = \"giga\"\n",
    "\n",
    "if task_name == \"giga\":\n",
    "    doc_name = \"/home/george/Projects/speechlab/pointer-generator/data/Giga/input.txt\"\n",
    "    summ_name = \"/home/george/Projects/speechlab/pointer-generator/data/Giga/task1_ref0.txt\"\n",
    "else:\n",
    "    doc_name = \"/home/george/Projects/speechlab/pointer-generator/data2/val.txt.src\"\n",
    "    summ_name = \"/home/george/Projects/speechlab/pointer-generator/data2/val.txt.tgt.tagged\"\n",
    "    \n",
    "val_size = 1951 if task_name == \"giga\" else 13368\n",
    "\n",
    "continue_from = \"trained-giga-50k/Model11\"\n",
    "eval_dir = \"evaluation-{}/\".format(task_name)\n",
    "data_seq_name = eval_dir+'tmp.json'\n",
    "vocab_name = 'data-{}/vocab.json'.format(task_name)\n",
    "\n",
    "!mkdir -p {eval_dir}\n",
    "\n",
    "num_threads = 4\n",
    "batch_size = 64 if task_name == \"giga\" else 16\n",
    "vocab = json.load(open(vocab_name, 'r'))\n",
    "VOC_SIZE = len(vocab)\n",
    "\n",
    "INPUT_MAX = 50 if task_name == \"giga\" else 400\n",
    "OUTPUT_MAX = 20 if task_name == \"giga\" else 100\n",
    "UNK = \"[UNK]\"\n",
    "BOS = \"[CLS]\"\n",
    "EOS = \"[SEP]\"\n",
    "PAD = \"[PAD]\"\n",
    "\n",
    "if task_name == 'giga':\n",
    "    token_mappings = {'UNK':UNK, '-lrb-':'(', '-rrb-':')'}\n",
    "else:\n",
    "    token_mappings = {'<unk>':UNK, '<t>':'', '</t>':'', '-lrb-':'(', '-rrb-':')'}\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "faster = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prepro = Preprocessor(doc_name, summ_name, 0, 50000, token_mappings, num_threads)\n",
    "summaries = prepro.summaries\n",
    "if faster:\n",
    "    prepro.vocab = vocab\n",
    "    prepro.vocab_inv = {a:b for b, a in vocab.items()}\n",
    "else:\n",
    "    prepro.process(vocab)\n",
    "    prepro.export(None,data_seq_name,None)\n",
    "    articles = prepro.documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not faster:\n",
    "    index = 73\n",
    "    print(\"[summary]\", summaries[index])\n",
    "    print(\"[documen]\", articles[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ev_set, ev_generator = make_data_generator(\\\n",
    "data_seq_name, INPUT_MAX, OUTPUT_MAX, vocab[PAD], batch_size, cutoff=None, shuffle=False, num_workers=4)\n",
    "\n",
    "def data_gen_val():\n",
    "    for src,tgt in ev_generator:\n",
    "        src = Variable(src, requires_grad=False).to(device)\n",
    "        tgt = Variable(tgt, requires_grad=False).to(device)\n",
    "        yield Batch(src, tgt, vocab[PAD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if str(device) == 'cpu':\n",
    "    saved_model = torch.load(continue_from, map_location=lambda storage, location: storage)\n",
    "else:\n",
    "    saved_model = torch.load(continue_from)\n",
    "\n",
    "model = make_model(VOC_SIZE, VOC_SIZE, N=4, d_model=256, d_ff=1024, h=8, dropout=0.1, emb_share=True)\n",
    "model.load_state_dict(saved_model['model'])\n",
    "model.eval()\n",
    "if str(device) != 'cpu':\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del saved_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode_batch(model, src, src_mask, max_len, start_symbol):\n",
    "    batch_size = src.shape[0]\n",
    "    \n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(batch_size, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask, \n",
    "                           Variable(ys), \n",
    "                           Variable(subsequent_mask(ys.size(1))\n",
    "                                    .type_as(src.data)))\n",
    "        #print(out.shape) 128,1,256\n",
    "        probs = model.generator(out[:, -1, :])\n",
    "        \n",
    "        #print(probs.shape) 128,30522\n",
    "        next_words = torch.argmax(probs, dim=1, keepdim=True)\n",
    "        \n",
    "        #print(next_words.shape)        \n",
    "        #print(ys.shape) both 128,1\n",
    "        \n",
    "        ys = torch.cat((ys, next_words), dim=1)\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readable(sent):\n",
    "    try:\n",
    "        end = sent.index(EOS)\n",
    "    except ValueError:\n",
    "        end = len(sent)\n",
    "    sent = [tok for tok in sent[:end] if tok not in [BOS, EOS, PAD]] # remove special tokens\n",
    "    sent = \" \".join(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = int(math.ceil(val_size / batch_size))\n",
    "\n",
    "hypothesis = []\n",
    "\n",
    "for i,batch in tqdm(enumerate(data_gen_val()), total= total):\n",
    "    srcs = batch.src\n",
    "    src_masks = batch.src_mask\n",
    "    \n",
    "    trgs = batch.trg\n",
    "    trg_masks = batch.trg_mask\n",
    "        \n",
    "    bs = srcs.shape[0]\n",
    "    \n",
    "    outs = greedy_decode_batch(model, srcs, src_masks, max_len=OUTPUT_MAX, start_symbol=vocab[BOS])\n",
    "    \n",
    "    for j, (out_tensor, tgt_tensor) in enumerate(zip(outs, trgs)):   \n",
    "        tokens = prepro.ids_to_tokens(out_tensor.cpu().numpy())\n",
    "        line = readable(tokens)\n",
    "        \n",
    "        hypothesis.append(line)\n",
    "            \n",
    "    if i == total:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, summaries, avg=True)\n",
    "print(scores['rouge-1']['f'], scores['rouge-2']['f'], scores['rouge-l']['f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# \n",
    "# \n",
    "# 0.3320365650729159 0.14976638992424993 0.29414807564498124\n",
    "# \n",
    "# \n",
    "# \n",
    "# 0.3401100694524329 0.1571963019013948 0.30353686782891715\n",
    "#\n",
    "#\n",
    "# 0.34232405091848905 0.16004304876386657 0.30398585868610345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 888\n",
    "print(hypothesis[index])\n",
    "print(summaries[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
