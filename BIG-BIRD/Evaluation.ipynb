{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge \n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessors import Preprocessor\n",
    "from dataset import make_data_generator\n",
    "from transformer_nb2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "task_name = \"giga\"\n",
    "\n",
    "if task_name == \"giga\":\n",
    "    doc_name = \"/home/george/Projects/speechlab/pointer-generator/data/Giga/input.txt\"\n",
    "    summ_name = \"/home/george/Projects/speechlab/pointer-generator/data/Giga/task1_ref0.txt\"\n",
    "else:\n",
    "    doc_name = \"/home/george/Projects/speechlab/pointer-generator/data2/val.txt.src\"\n",
    "    summ_name = \"/home/george/Projects/speechlab/pointer-generator/data2/val.txt.tgt.tagged\"\n",
    "    \n",
    "val_size = 1951 if task_name == \"giga\" else 13368\n",
    "\n",
    "continue_from = \"trained-giga-50k/Model11\"\n",
    "eval_dir = \"evaluation-{}/\".format(task_name)\n",
    "data_seq_name = eval_dir+'tmp.json'\n",
    "vocab_name = 'data-{}/vocab.json'.format(task_name)\n",
    "\n",
    "!mkdir -p {eval_dir}\n",
    "\n",
    "num_threads = 4\n",
    "batch_size = 64 if task_name == \"giga\" else 16\n",
    "vocab = json.load(open(vocab_name, 'r'))\n",
    "VOC_SIZE = len(vocab)\n",
    "\n",
    "INPUT_MAX = 50 if task_name == \"giga\" else 400\n",
    "OUTPUT_MAX = 20 if task_name == \"giga\" else 100\n",
    "UNK = \"[UNK]\"\n",
    "BOS = \"[CLS]\"\n",
    "EOS = \"[SEP]\"\n",
    "PAD = \"[PAD]\"\n",
    "\n",
    "if task_name == 'giga':\n",
    "    token_mappings = {'UNK':UNK, '-lrb-':'(', '-rrb-':')'}\n",
    "else:\n",
    "    token_mappings = {'<unk>':UNK, '<t>':'', '</t>':'', '-lrb-':'(', '-rrb-':')'}\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "faster = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f7b3ee006c41efb55425f9ac69a009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1951), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e7726150114aed8df8a847bd69fb9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1951), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[info] making vocabulary...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e5a163cd9f40ca81e03024158f4581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1951), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7d2157e507425b801d54da106e375d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1951), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[info] using external vocabulary !!!\n",
      "[info] converting to indices...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bf526cbea240b9812774c916ed0e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1951), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4642febad3d34868895f2c93a708c18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1951), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[info] dumping training data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prepro = Preprocessor(doc_name, summ_name, 0, 50000, token_mappings, num_threads)\n",
    "summaries = prepro.summaries\n",
    "if faster:\n",
    "    prepro.vocab = vocab\n",
    "    prepro.vocab_inv = {a:b for b, a in vocab.items()}\n",
    "else:\n",
    "    prepro.process(vocab)\n",
    "    prepro.export(None,data_seq_name,None)\n",
    "    articles = prepro.documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[summary] russia warns of colossal impact if nato takes in ukraine georgia [UNK] [UNK] quote\n",
      "[documen] ['russia', 'warned', 'wednesday', 'against', 'nato', 'taking', 'in', 'the', 'ex-soviet', 'republics', 'of', 'ukraine', 'and', 'georgia', ',', 'saying', 'such', 'a', 'colossal', 'geopolitical', 'shift', 'would', 'threaten', 'relations', '.']\n"
     ]
    }
   ],
   "source": [
    "if not faster:\n",
    "    index = 73\n",
    "    print(\"[summary]\", summaries[index])\n",
    "    print(\"[documen]\", articles[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading json\n",
      "load json done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85659038b224f848875738ee61010ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1951), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ev_set, ev_generator = make_data_generator(\\\n",
    "data_seq_name, INPUT_MAX, OUTPUT_MAX, vocab[PAD], batch_size, cutoff=None, shuffle=False, num_workers=4)\n",
    "\n",
    "def data_gen_val():\n",
    "    for src,tgt in ev_generator:\n",
    "        src = Variable(src, requires_grad=False).to(device)\n",
    "        tgt = Variable(tgt, requires_grad=False).to(device)\n",
    "        yield Batch(src, tgt, vocab[PAD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if str(device) == 'cpu':\n",
    "    saved_model = torch.load(continue_from, map_location=lambda storage, location: storage)\n",
    "else:\n",
    "    saved_model = torch.load(continue_from)\n",
    "\n",
    "model = make_model(VOC_SIZE, VOC_SIZE, N=4, d_model=256, d_ff=1024, h=8, dropout=0.1, emb_share=True)\n",
    "model.load_state_dict(saved_model['model'])\n",
    "model.eval()\n",
    "if str(device) != 'cpu':\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del saved_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode_batch(model, src, src_mask, max_len, start_symbol):\n",
    "    batch_size = src.shape[0]\n",
    "    \n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(batch_size, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask, \n",
    "                           Variable(ys), \n",
    "                           Variable(subsequent_mask(ys.size(1))\n",
    "                                    .type_as(src.data)))\n",
    "        #print(out.shape) 128,1,256\n",
    "        probs = model.generator(out[:, -1, :])\n",
    "        \n",
    "        #print(probs.shape) 128,30522\n",
    "        next_words = torch.argmax(probs, dim=1, keepdim=True)\n",
    "        \n",
    "        #print(next_words.shape)        \n",
    "        #print(ys.shape) both 128,1\n",
    "        \n",
    "        ys = torch.cat((ys, next_words), dim=1)\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readable(sent):\n",
    "    try:\n",
    "        end = sent.index(EOS)\n",
    "    except ValueError:\n",
    "        end = len(sent)\n",
    "    sent = [tok for tok in sent[:end] if tok not in [BOS, EOS, PAD]] # remove special tokens\n",
    "    sent = \" \".join(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e3790b126548c891c03034452e8dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total = int(math.ceil(val_size / batch_size))\n",
    "\n",
    "hypothesis = []\n",
    "\n",
    "for i,batch in tqdm(enumerate(data_gen_val()), total= total):\n",
    "    srcs = batch.src\n",
    "    src_masks = batch.src_mask\n",
    "    \n",
    "    trgs = batch.trg\n",
    "    trg_masks = batch.trg_mask\n",
    "        \n",
    "    bs = srcs.shape[0]\n",
    "    \n",
    "    outs = greedy_decode_batch(model, srcs, src_masks, max_len=OUTPUT_MAX, start_symbol=vocab[BOS])\n",
    "    \n",
    "    for j, (out_tensor, tgt_tensor) in enumerate(zip(outs, trgs)):   \n",
    "        tokens = prepro.ids_to_tokens(out_tensor.cpu().numpy())\n",
    "        line = readable(tokens)\n",
    "        \n",
    "        hypothesis.append(line)\n",
    "            \n",
    "    if i == total:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34232405091848905 0.16004304876386657 0.30398585868610345\n"
     ]
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, summaries, avg=True)\n",
    "print(scores['rouge-1']['f'], scores['rouge-2']['f'], scores['rouge-l']['f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# \n",
    "# \n",
    "# 0.3320365650729159 0.14976638992424993 0.29414807564498124\n",
    "# \n",
    "# \n",
    "# \n",
    "# 0.3401100694524329 0.1571963019013948 0.30353686782891715\n",
    "#\n",
    "#\n",
    "# 0.34232405091848905 0.16004304876386657 0.30398585868610345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokyo stocks end higher dollar down against yen\n",
      "stocks bounce back after two-day decline dollar mixed eds : [UNK] up with late dollar closing bond prices comments\n"
     ]
    }
   ],
   "source": [
    "index = 888\n",
    "print(hypothesis[index])\n",
    "print(summaries[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
