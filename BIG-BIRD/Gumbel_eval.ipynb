{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RelGAN import *\n",
    "from dataset import make_data_generator\n",
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'data/IMDB/'\n",
    "data_name = folder+'data.json'\n",
    "# validation_name = folder+'valid_seq.json'\n",
    "# testdata_name = folder+'testdata_seq.json'\n",
    "vocab_name = folder+'vocab.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = json.load(open(vocab_name, 'r'))\n",
    "VOC_SIZE = len(vocab)\n",
    "INPUT_MAX = 150\n",
    "SUMM_MAX = 50\n",
    "UNK = \"[UNK]\"\n",
    "BOS = \"[CLS]\"\n",
    "EOS = \"[SEP]\"\n",
    "PAD = \"[PAD]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading json\n",
      "load json done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21052f593a5947acbe3e99afa9a0c736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "training_set, training_generator = make_data_generator(\\\n",
    "data_name, INPUT_MAX, SUMM_MAX, vocab[PAD], batch_size, cutoff=None, shuffle=True, num_workers=4)\n",
    "\n",
    "# validation_set, validation_generator = make_data_generator(\\\n",
    "# validation_name, INPUT_MAX, OUTPUT_MAX, vocab[PAD], batch_size, cutoff=validation_size, shuffle=False, num_workers=4)\n",
    "\n",
    "def data_gen_train():\n",
    "    for src, label, tgt in training_generator:\n",
    "        src = src.to(device)\n",
    "        #label = (label).long().to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        b = Batch(src, tgt, vocab[PAD])\n",
    "        #b.label = label\n",
    "        yield b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "total_train = int(math.ceil(training_set.size / batch_size))\n",
    "# total_valid = int(math.ceil(validation_set.size / batch_size))\n",
    "# print(total_train, total_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_param(model):\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "def make_big_bird(vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1, emb_share=False, bert_share=False):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    \n",
    "    vocab_sz = len(vocab)\n",
    "        \n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    \n",
    "    src_emb = nn.Sequential(Embeddings(d_model, vocab_sz), c(position))\n",
    "    if emb_share:        \n",
    "        tgt_emb = src_emb\n",
    "        bert_class_emb = src_emb\n",
    "        bert_discr_emb = src_emb\n",
    "    else:\n",
    "        tgt_emb = nn.Sequential(Embeddings(d_model, vocab_sz), c(position))\n",
    "        bert_class_emb = nn.Sequential(Embeddings(d_model, vocab_sz), c(position))\n",
    "        bert_discr_emb = nn.Sequential(Embeddings(d_model, vocab_sz), c(position))\n",
    "        \n",
    "    translator = RelationalMemory(\n",
    "        mem_slots = 1,\n",
    "        head_size = 192,\n",
    "        input_size = d_model,\n",
    "        num_tokens = vocab_sz,\n",
    "        device = device,\n",
    "        num_heads = 4,\n",
    "        attention_mlp_layers=3,\n",
    "        key_size = 64,\n",
    "        use_adaptive_softmax=True,\n",
    "        cutoffs = [1000, 5000, 20000]\n",
    "    )\n",
    "\n",
    "    reconstructor = RelationalMemory(\n",
    "        mem_slots = 1,\n",
    "        head_size = 192,\n",
    "        input_size = d_model,\n",
    "        num_tokens = vocab_sz,\n",
    "        device = device,\n",
    "        num_heads = 4,\n",
    "        attention_mlp_layers=3,\n",
    "        key_size = 64,\n",
    "        use_adaptive_softmax=True,\n",
    "        cutoffs = [1000, 5000, 20000]\n",
    "    )\n",
    "#     reconstructor = LSTM_Normal_Encoder_Decoder(\n",
    "#         hidden_dim=d_model, \n",
    "#         emb_dim=d_model, \n",
    "#         input_len=SUMM_MAX, \n",
    "#         output_len=INPUT_MAX, \n",
    "#         voc_size=vocab_sz, \n",
    "#         pad_index=vocab[PAD],\n",
    "#         device = device,\n",
    "#         eps=1e-8,\n",
    "#         num_layers = 2\n",
    "#     )\n",
    " \n",
    "    discriminator = Discriminator(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        d_model,\n",
    "        len(vocab),\n",
    "        vocab[PAD]\n",
    "    )\n",
    "\n",
    "\n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for m in [translator, reconstructor, discriminator]:\n",
    "        init_param(m)\n",
    "            \n",
    "    # creation of big bird\n",
    "    model = BigBird(\n",
    "        translator, discriminator, reconstructor , \n",
    "        vocab, gamma=0.99, clip_value=0.5, #for WGAN, useless if WGAN-GP is used \n",
    "        lr_G = 5e-5,\n",
    "        lr_D = 1e-4,\n",
    "        lr_R = 2e-4,\n",
    "        LAMBDA = 10, # Gradient penalty lambda hyperparameter\n",
    "        TEMP_END = 0.8,\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load Bird from Nest/DoubleRelationMEM_GAN\n"
     ]
    }
   ],
   "source": [
    "model = make_big_bird(vocab, N=4, d_model=256, d_ff=256, h=4, dropout=0.1, emb_share=True, bert_share=True)\n",
    "model.load(\"Nest/DoubleRelationMEM_GAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_inv = {a:b for b, a in vocab.items()}\n",
    "def convert_ids_to_tokens(ids):\n",
    "    return [vocab_inv[i] for i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876bc4a632a34d9b9316b5013775050e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin:\n",
      "[CLS] an absolute classic ! ! the direction is flawless , the acting is just superb . words fall short for this great work . the most definitive movie on mumbai police . this movie has stood the test of times . om puri gives a stellar performance , smita patil no less . all the actors have done their best and the movie races on thrilling you at every moment . this movie shakes your whole being badly and forces you to rethink about many issues that confront our society . this is the story of a cop ( om puri ) who starts out in his career as a honest man but ultimately degenerates into a killer . the first attempt in bollywood to get behind the scenes and expose the depressing truth about mumbai cops . kudos to nihalani ! ! after this movie a slew of\n",
      "summary:\n",
      "[CLS] brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally brutally\n",
      "real summary:\n",
      "[CLS] an absolute classic ! ! the direction is flawless , the acting is just superb . words fall short for this great work . the most definitive movie on mumbai police . this movie has stood the test of times . om puri gives a stellar performance ,\n",
      "reconsturct out:\n",
      "i interesting film film ! ! movie is not . the story is very very , the are from and the movie movie of the acting part movie i the , is . is is a out same of the . [SEP] and is a very performance as and and , character . the the actors are been the roles . the movie is . the . will all time . the is is the heart time a done the the to the what the other . are the own . [SEP] is a best of a young who who fatale ) who is to to the own . a young . who he is into a very . he only part to the and the the the film of the the story and of the and . the to the , [SEP] ! the , , lot of\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trange = tqdm(enumerate(data_gen_train()), total=total_train)\n",
    "all_acc = [];\n",
    "all_CE_loss = [];\n",
    "ct = 0\n",
    "for i, batch in trange:\n",
    "    acc, loss = model.eval_iter(batch.src, batch.src_mask, SUMM_MAX, batch.trg, ct)\n",
    "    all_acc.append(acc)\n",
    "    all_CE_loss.append(loss)\n",
    "    ct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(all_CE_loss)), all_CE_loss)\n",
    "print(\"[eval] epoch reward avg:\",sum(all_CE_loss)/len(all_CE_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(all_acc)), all_acc)\n",
    "print(\"[eval] epoch acc avg:\",sum(all_acc)/len(all_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
