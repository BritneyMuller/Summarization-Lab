{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessors import Preprocessor\n",
    "# import json\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "# from multiprocessing import Pool\n",
    "\n",
    "# UNK = \"[UNK]\"\n",
    "# BOS = \"[CLS]\"\n",
    "# EOS = \"[SEP]\"\n",
    "# PAD = \"[PAD]\"\n",
    "\n",
    "# class Preprocessor:\n",
    "#     def __init__(self, doc_name, summ_name, validation_split, vocab_size, token_mappings, num_threads):\n",
    "#         self.summaries = []\n",
    "#         self.documents = []\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.token_mappings = token_mappings\n",
    "#         self.threads = num_threads\n",
    "#         self.validation_split = validation_split\n",
    "\n",
    "#         with open(doc_name, newline='', encoding='utf-8') as f:\n",
    "#             total = self.getlines(doc_name)\n",
    "#             for i,line in tqdm(enumerate(f), total=total):\n",
    "#                 line = self.swap(line.strip())\n",
    "#                 self.documents.append(line)\n",
    "#         with open(summ_name, newline='', encoding='utf-8') as f:\n",
    "#             total = self.getlines(summ_name)\n",
    "#             for i,line in tqdm(enumerate(f), total=total):\n",
    "#                 line = self.swap(line.strip())\n",
    "#                 self.summaries.append(line)\n",
    "                \n",
    "#         self.size = len(self.summaries)\n",
    "        \n",
    "    \n",
    "#     def process(self):\n",
    "#         print(\"[info] making vocabulary...\")\n",
    "#         self.make_vocab()\n",
    "#         print(\"[info] converting to indices...\")\n",
    "#         self.convert_all_to_ids()  \n",
    "                \n",
    "#     def make_vocab(self):\n",
    "#         sum_toks = []\n",
    "#         doc_toks = []\n",
    "#         vocab = {BOS:99999, EOS:99999, PAD:99999, UNK:99999}\n",
    "\n",
    "#         for d in tqdm(self.summaries):\n",
    "#             ts = d.split()\n",
    "#             for t in ts:\n",
    "#                 vocab[t] = vocab.get(t, 0) + 1\n",
    "#             sum_toks.append(ts)\n",
    "\n",
    "#         for d in tqdm(self.documents):\n",
    "#             ts = d.split()\n",
    "#             for t in ts:\n",
    "#                 vocab[t] = vocab.get(t, 0) + 1\n",
    "#             doc_toks.append(ts)\n",
    "            \n",
    "#         vocab_sort = sorted(vocab.items(), key=lambda x: -x[1]) # descending\n",
    "        \n",
    "#         self.vocab = { v:i for i, (v, n) in enumerate(vocab_sort[:self.vocab_size])}\n",
    "#         self.summaries = sum_toks\n",
    "#         self.documents = doc_toks\n",
    "        \n",
    "#     def tokens_to_ids(self, s):\n",
    "#         return [self.vocab[BOS]] + [self.vocab.get(t, self.vocab[UNK]) for t in s] + [self.vocab[EOS]]\n",
    "    \n",
    "#     def convert_all_to_ids(self):\n",
    "#         if self.threads < 2:\n",
    "#             self.summ_seqs = [self.tokens_to_ids(s) for s in tqdm(self.summaries)]\n",
    "#             self.docu_seqs = [self.tokens_to_ids(s) for s in tqdm(self.documents)]\n",
    "#         else:\n",
    "#             self.summ_seqs = [self.tokens_to_ids(s) for s in tqdm(self.summaries)]\n",
    "#             self.docu_seqs = [self.tokens_to_ids(s) for s in tqdm(self.documents)]\n",
    "    \n",
    "#     def export(self, vocab_name, data_seq_name, valid_seq_name):\n",
    "#         print(\"[info] dumping vocab...\")\n",
    "#         json.dump(self.vocab, open(vocab_name, 'w'))\n",
    "        \n",
    "#         seqdata = {'summary':[], 'document':[]}\n",
    "#         valseqdata = {'summary':[], 'document':[]}\n",
    "        \n",
    "#         print(\"[info] splitting data...\")\n",
    "#         num_summ = self.size\n",
    "#         val_set = np.random.randint(0, num_summ, size=int(self.validation_split*num_summ))\n",
    "#         for i in range(num_summ):\n",
    "#             if i in val_set:\n",
    "#                 valseqdata['summary'].append(self.summ_seqs[i])\n",
    "#                 valseqdata['document'].append(self.docu_seqs[i])\n",
    "#             else:\n",
    "#                 seqdata['summary'].append(self.summ_seqs[i])\n",
    "#                 seqdata['document'].append(self.docu_seqs[i])\n",
    "        \n",
    "#         print(\"[info] dumping training data...\")\n",
    "#         json.dump(seqdata, open(data_seq_name, 'w'))\n",
    "        \n",
    "#         print(\"[info] dumping validation data...\")\n",
    "#         json.dump(valseqdata, open(valid_seq_name, 'w'))\n",
    "        \n",
    "        \n",
    "#     def swap(self,s):\n",
    "#         for t, t_p in self.token_mappings.items():\n",
    "#             s = s.replace(t, t_p)\n",
    "#         if s == \"\":\n",
    "#             s = UNK\n",
    "#         return s\n",
    "    \n",
    "#     def getlines(self,name):\n",
    "#         total = !wc -l {name}\n",
    "#         return int(total[0].split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"giga\"\n",
    "\n",
    "if task_name == \"giga\":\n",
    "    doc_name = \"../pointer-generator/data/train.article.txt\"\n",
    "    summ_name = \"../pointer-generator/data/train.title.txt\"\n",
    "else:\n",
    "    doc_name = \"../pointer-generator/data2/train.txt.src\"\n",
    "    summ_name = \"../pointer-generator/data2/train.txt.tgt.tagged\"\n",
    "\n",
    "out_dir = \"data-{}/\".format(task_name)\n",
    "vocab_name = out_dir+\"vocab.json\"\n",
    "data_seq_name = out_dir+\"train_seq.json\"\n",
    "valid_seq_name = out_dir+\"valid_seq.json\"\n",
    "\n",
    "num_threads = 4\n",
    "corpus_size = 88888888\n",
    "validation_split = 0.002\n",
    "\n",
    "if task_name == 'giga':\n",
    "    token_mappings = {'<unk>':UNK, '-lrb-':'(', '-rrb-':')'}\n",
    "else:\n",
    "    token_mappings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {out_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae08f53322f14e6c9e0f0b109d91b921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3803957), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0ff29ad766422293aa1f98f23eb083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3803957), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "p = Preprocessor(doc_name, summ_name, validation_split, 50000, token_mappings, num_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] making vocabulary...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9da4a4532f44abeb1a06e38d8e209b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3803957), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d20b8a0e0ef4baaa4e157001f18326a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3803957), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[info] converting to indices...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b6ab29e339483b8d87836bb7b6736e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3803957), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0960d79d089641ea8fa42aa89ce51d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3803957), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "p.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] dumping vocab...\n",
      "[info] splitting data...\n",
      "[info] dumping training data...\n",
      "[info] dumping validation data...\n"
     ]
    }
   ],
   "source": [
    "p.export(vocab_name,data_seq_name,valid_seq_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
