{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RelGAN import *\n",
    "from dataset import make_data_generator\n",
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "#from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'data/IMDB/'\n",
    "data_name = folder+'data.json'\n",
    "# validation_name = folder+'valid_seq.json'\n",
    "# testdata_name = folder+'testdata_seq.json'\n",
    "vocab_name = folder+'vocab.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "save_rate = 1 #how many epochs per modelsave\n",
    "#continue_from = \"trained/Model1\" # if none, put None\n",
    "continue_from = None\n",
    "epsilon = 1e-8\n",
    "validation_size = 10000\n",
    "device = torch.device('cuda')\n",
    "!mkdir -p trained\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'LSTM_GumbelSoftmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = json.load(open(vocab_name, 'r'))\n",
    "VOC_SIZE = len(vocab)\n",
    "INPUT_MAX = 150\n",
    "SUMM_MAX = 50\n",
    "UNK = \"[UNK]\"\n",
    "BOS = \"[CLS]\"\n",
    "EOS = \"[SEP]\"\n",
    "PAD = \"[PAD]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading json\n",
      "load json done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2f25cbf12d4dec9ad8054445ecdeb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "training_set, training_generator = make_data_generator(\\\n",
    "data_name, INPUT_MAX, SUMM_MAX, vocab[PAD], batch_size, cutoff=None, shuffle=True, num_workers=4)\n",
    "\n",
    "# validation_set, validation_generator = make_data_generator(\\\n",
    "# validation_name, INPUT_MAX, OUTPUT_MAX, vocab[PAD], batch_size, cutoff=validation_size, shuffle=False, num_workers=4)\n",
    "\n",
    "def data_gen_train():\n",
    "    for src, label, tgt in training_generator:\n",
    "        src = src.to(device)\n",
    "        #label = (label).long().to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        b = Batch(src, tgt, vocab[PAD])\n",
    "        #b.label = label\n",
    "        yield b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "total_train = int(math.ceil(training_set.size / batch_size))\n",
    "# total_valid = int(math.ceil(validation_set.size / batch_size))\n",
    "# print(total_train, total_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_param(model):\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "def make_big_bird(vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1, emb_share=False, bert_share=False):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    \n",
    "    vocab_sz = len(vocab)\n",
    "        \n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    \n",
    "    src_emb = nn.Sequential(Embeddings(d_model, vocab_sz), c(position))\n",
    "    if emb_share:        \n",
    "        tgt_emb = src_emb\n",
    "        bert_class_emb = src_emb\n",
    "        bert_discr_emb = src_emb\n",
    "    else:\n",
    "        tgt_emb = nn.Sequential(Embeddings(d_model, vocab_sz), c(position))\n",
    "        bert_class_emb = nn.Sequential(Embeddings(d_model, vocab_sz), c(position))\n",
    "        bert_discr_emb = nn.Sequential(Embeddings(d_model, vocab_sz), c(position))\n",
    "        \n",
    "    translator = RelationalMemory(\n",
    "        mem_slots = 1,\n",
    "        head_size = 192,\n",
    "        input_size = d_model,\n",
    "        num_tokens = vocab_sz,\n",
    "        device = device,\n",
    "        num_heads = 4,\n",
    "        attention_mlp_layers=3,\n",
    "        key_size = 64,\n",
    "        use_adaptive_softmax=True,\n",
    "        cutoffs = [1000, 5000, 20000]\n",
    "    )\n",
    "\n",
    "    reconstructor = RelationalMemory(\n",
    "        mem_slots = 1,\n",
    "        head_size = 192,\n",
    "        input_size = d_model,\n",
    "        num_tokens = vocab_sz,\n",
    "        device = device,\n",
    "        num_heads = 4,\n",
    "        attention_mlp_layers=3,\n",
    "        key_size = 64,\n",
    "        use_adaptive_softmax=True,\n",
    "        cutoffs = [1000, 5000, 20000]\n",
    "    )\n",
    "#     reconstructor = LSTM_Normal_Encoder_Decoder(\n",
    "#         hidden_dim=d_model, \n",
    "#         emb_dim=d_model, \n",
    "#         input_len=SUMM_MAX, \n",
    "#         output_len=INPUT_MAX, \n",
    "#         voc_size=vocab_sz, \n",
    "#         pad_index=vocab[PAD],\n",
    "#         device = device,\n",
    "#         eps=1e-8,\n",
    "#         num_layers = 2\n",
    "#     )\n",
    " \n",
    "    discriminator = Discriminator(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        d_model,\n",
    "        len(vocab),\n",
    "        vocab[PAD]\n",
    "    )\n",
    "\n",
    "\n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for m in [translator, reconstructor, discriminator]:\n",
    "        init_param(m)\n",
    "            \n",
    "    # creation of big bird\n",
    "    model = BigBird(\n",
    "        translator, discriminator, reconstructor , \n",
    "        vocab, gamma=0.99, clip_value=0.5, #for WGAN, useless if WGAN-GP is used \n",
    "        lr_G = 5e-5,\n",
    "        lr_D = 1e-4,\n",
    "        lr_R = 5e-5,\n",
    "        LAMBDA = 10, # Gradient penalty lambda hyperparameter\n",
    "        TEMP_END = 0.5,\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_big_bird(vocab, N=4, d_model=256, d_ff=256, h=4, dropout=0.1, emb_share=True, bert_share=True)\n",
    "#model.load(\"Nest/NewbornBird_LSTM_GumbelSoftmax\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_inv = {a:b for b, a in vocab.items()}\n",
    "def convert_ids_to_tokens(ids):\n",
    "    return [vocab_inv[i] for i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Not authenticated.  Copy a key from https://app.wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key: ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: /home/marvin/.netrc\n",
      "wandb: ERROR Unable to read ~/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/marvin852316497/seq2seq-discrete-encoder-decoder/runs/0o2v4n6l?apiKey=ecc70f422dabf793a9101343c84e8ead3c0bf72e\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
       "        in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<wandb.wandb_torch.TorchGraph at 0x7f8b437dacc0>,\n",
       " <wandb.wandb_torch.TorchGraph at 0x7f8b425bbc50>,\n",
       " <wandb.wandb_torch.TorchGraph at 0x7f8b425bb438>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "wandb.init(project=\"seq2seq-discrete-encoder-decoder\")\n",
    "wandb.config.update({\n",
    "    \"batch_size\": batch_size,\n",
    "    \"input len\":INPUT_MAX,\n",
    "    \"summary len\":SUMM_MAX,\n",
    "    \"lr_G\":model.lr_G,\n",
    "    \"lr_D\":model.lr_D,\n",
    "    \"lr_R\":model.lr_R,\n",
    "    \"temperature min\":model.TEMP_END,\n",
    "    })\n",
    "wandb.watch([model.generator, model.discriminator, model.reconstructor])\n",
    "#ecc70f422dabf793a9101343c84e8ead3c0bf72e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2aa07ab71c48afba6425b1010ad159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3125), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lay egg to ./Nest ... save as ./Nest/DoubleRelationMEM_GAN\n"
     ]
    }
   ],
   "source": [
    "#start = 1 if continue_from == None else (int(continue_from.split(\"Model\")[-1])+1)\n",
    "history = []\n",
    "\n",
    "start = model.epoch\n",
    "\n",
    "#from tensorboardX import SummaryWriter\n",
    "#writer = SummaryWriter('mygraph')\n",
    "\n",
    "step_ct = 1\n",
    "width = 0.35\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(start, num_epochs+1):\n",
    "    print(\"Epoch\", epoch)\n",
    "    \n",
    "    # training\n",
    "    stats = Stats()\n",
    "    model.train()\n",
    "    \n",
    "    rewards = []\n",
    "    \n",
    "    trange = tqdm(enumerate(data_gen_train()), total=total_train)\n",
    "    for i, batch in trange:\n",
    "        #expect src has [CLS] and [SEP]\n",
    "        GAN_loss, Rec_loss, score, output, distrib, one_hot = model.run_iter(batch.src, batch.src_mask, SUMM_MAX, batch.trg, D_iters=5, verbose = 0)\n",
    "        trange.set_postfix(\n",
    "            **{'G_loss': '{:.3f}'.format(GAN_loss[0])},\n",
    "            **{'D_loss': '{:.3f}'.format(GAN_loss[1])},\n",
    "            **{'CE_loss': '{:.3f}'.format(Rec_loss[0])},\n",
    "            #**{'vq_loss': '{:.3f}'.format(Rec_loss[1])},\n",
    "            #**{'commit_loss': '{:.3f}'.format(Rec_loss[2])},\n",
    "            **{'real_score': '{:.3f}'.format(score[0])},\n",
    "            **{'fake_score': '{:.3f}'.format(score[1])},\n",
    "            **{'acc': '{:.3f}'.format(score[2])},\n",
    "        )\n",
    "\n",
    "        if step_ct % 50 == 0:\n",
    "            \n",
    "            x = np.arange(len(distrib))\n",
    "            ratio = 1.0/max(distrib)\n",
    "            plt.bar(x, ratio * distrib, label='distrib hist' , align = \"edge\", width = width)\n",
    "            plt.bar(x, one_hot, label='Gumbel softmax hist' , align = \"edge\", width = -width)\n",
    "            \n",
    "            plt.legend()\n",
    "            plt.title(\"distrib vs gumbel sample (max distrib is upscale to 1)\")\n",
    "            plt.xlabel(\"dictionary [:100]\")\n",
    "            plt.ylabel(\"prob\")\n",
    "            wandb.log({\"hist\":wandb.Image(plt)})\n",
    "            plt.clf()\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "        wandb.log({\"input\":output[0],\n",
    "                   \"encode out\":output[1],\n",
    "                   \"reconsturct out\":output[2],        \n",
    "                  })\n",
    "        wandb.log({\n",
    "                   \"G_loss\":GAN_loss[0],\n",
    "                   \"D_loss\":GAN_loss[1],\n",
    "                   \"CE_loss\":Rec_loss[0],\n",
    "                   #\"vq_loss\":Rec_loss[1],\n",
    "                   #\"commit_loss\":Rec_loss[2],\n",
    "                   \"real_score\":score[0],\n",
    "                   \"fake_score\":score[1],\n",
    "                   \"acc\":score[2],\n",
    "                   \"gumbel temperature\":model.gumbel_temperature\n",
    "                  }, commit=False)\n",
    "            \n",
    "        step_ct += 1\n",
    "    model.epoch += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(all_loss)), all_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model.all_rewards)\n",
    "#plt.plot(range(len(model.all_rewards)), model.all_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(range(len(model.all_rewards)-1), [sum(model.all_rewards[:i])/i for i in range(1,len(model.all_rewards))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if appear [enforce fail at CPUAllocator.cpp:56], it means cutoffs of adaptive softmax is too big\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
