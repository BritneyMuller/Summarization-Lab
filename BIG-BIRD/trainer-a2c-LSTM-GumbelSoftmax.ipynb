{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LSTM_BigBirdA2C_GumbelSoftmax import *\n",
    "from dataset import make_data_generator\n",
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "#from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/tmp2/Food/'\n",
    "data_name = folder+'data.json'\n",
    "# validation_name = folder+'valid_seq.json'\n",
    "# testdata_name = folder+'testdata_seq.json'\n",
    "vocab_name = folder+'vocab.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "save_rate = 1 #how many epochs per modelsave\n",
    "#continue_from = \"trained/Model1\" # if none, put None\n",
    "continue_from = None\n",
    "epsilon = 1e-8\n",
    "validation_size = 10000\n",
    "device = torch.device('cuda')\n",
    "!mkdir -p trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = json.load(open(vocab_name, 'r'))\n",
    "VOC_SIZE = len(vocab)\n",
    "INPUT_MAX = 100\n",
    "SUMM_MAX = 20\n",
    "UNK = \"[UNK]\"\n",
    "BOS = \"[CLS]\"\n",
    "EOS = \"[SEP]\"\n",
    "PAD = \"[PAD]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading json\n",
      "load json done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf33800bca314a9298cee4ad091a20a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=568454), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "training_set, training_generator = make_data_generator(\\\n",
    "data_name, INPUT_MAX, SUMM_MAX, vocab[PAD], batch_size, cutoff=None, shuffle=True, num_workers=4)\n",
    "\n",
    "# validation_set, validation_generator = make_data_generator(\\\n",
    "# validation_name, INPUT_MAX, OUTPUT_MAX, vocab[PAD], batch_size, cutoff=validation_size, shuffle=False, num_workers=4)\n",
    "\n",
    "def data_gen_train():\n",
    "    for src, label, tgt in training_generator:\n",
    "        src = src.to(device)\n",
    "        label = (label).long().to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        b = Batch(src, tgt, vocab[PAD])\n",
    "        b.label = label\n",
    "        yield b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "total_train = int(math.ceil(training_set.size / batch_size))\n",
    "# total_valid = int(math.ceil(validation_set.size / batch_size))\n",
    "# print(total_train, total_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_param(model):\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "def make_big_bird(vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1, emb_share=False, bert_share=False):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    \n",
    "    vocab_sz = len(vocab)\n",
    "    \n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    \n",
    "    src_emb = nn.Sequential(Embeddings(d_model, vocab_sz), c(position))\n",
    "    if emb_share:        \n",
    "        tgt_emb = src_emb\n",
    "        bert_class_emb = src_emb\n",
    "        bert_discr_emb = src_emb\n",
    "    else:\n",
    "        tgt_emb = nn.Sequential(Embeddings(d_model, vocab_sz), c(position))\n",
    "        bert_class_emb = nn.Sequential(Embeddings(d_model, vocab_sz), c(position))\n",
    "        bert_discr_emb = nn.Sequential(Embeddings(d_model, vocab_sz), c(position))\n",
    "    \n",
    "    \n",
    "    bert_class = BERT(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        bert_class_emb,\n",
    "        vocab[PAD]\n",
    "    )\n",
    "    \n",
    "    if bert_share:\n",
    "        bert_discr = bert_class\n",
    "    else:\n",
    "        bert_discr = BERT(\n",
    "            Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "            bert_discr_emb,\n",
    "            vocab[PAD]\n",
    "        )\n",
    "        \n",
    "    translator = LSTM_Gumbel_Encoder_Decoder(\n",
    "        hidden_dim=d_model, \n",
    "        emb_dim=d_model, \n",
    "        input_len=INPUT_MAX, \n",
    "        output_len=SUMM_MAX, \n",
    "        voc_size=vocab_sz, \n",
    "        critic_net=CriticNet(2*d_model),\n",
    "        device=device,\n",
    "        eps=1e-8\n",
    "    )\n",
    "    \n",
    "    reconstructor = LSTM_Normal_Encoder_Decoder(\n",
    "        hidden_dim=d_model, \n",
    "        emb_dim=d_model, \n",
    "        input_len=SUMM_MAX, \n",
    "        output_len=INPUT_MAX, \n",
    "        voc_size=vocab_sz, \n",
    "        pad_index=vocab[PAD],\n",
    "        eps=1e-8\n",
    "    )\n",
    "#     translator = Translator(\n",
    "#         Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "#         Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "#                              c(ff), dropout), N),\n",
    "#         src_emb,\n",
    "#         tgt_emb,\n",
    "#         Generator(d_model, vocab_sz, device),\n",
    "#         CriticNet(d_model)\n",
    "#         )\n",
    "    \n",
    "#     classifier = Classifier(\n",
    "#         bert_class,\n",
    "#         out_class = 5\n",
    "#         # criterion = BCE\n",
    "#     )\n",
    "#     reconstructor = Reconstructor(\n",
    "#         Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "#         Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "#                              c(ff), dropout), N),\n",
    "#         src_emb,\n",
    "#         tgt_emb,\n",
    "#         Generator(d_model, vocab_sz, device),\n",
    "#         vocab[PAD]\n",
    "#     )   \n",
    "    discriminator = Discriminator(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        d_model,\n",
    "        len(vocab),\n",
    "        vocab[PAD]\n",
    "    )\n",
    "\n",
    "\n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for m in [translator, reconstructor, discriminator]:\n",
    "        init_param(m)\n",
    "        \n",
    "#     if(str(device) == 'cpu'):\n",
    "#         savedmodel = torch.load(\"pretrained/Translator4\", map_location=lambda storage, location: storage)\n",
    "#     else:\n",
    "#         savedmodel = torch.load(\"pretrained/Translator4\")\n",
    "#     translator.load_state_dict(savedmodel['model'], strict=False)\n",
    "#     if(str(device) == 'cpu'):\n",
    "#         savedmodel = torch.load(\"pretrained/Translator4\", map_location=lambda storage, location: storage)\n",
    "#     else:\n",
    "#         savedmodel = torch.load(\"pretrained/Translator4\")\n",
    "#     reconstructor.load_state_dict(savedmodel['model'])#, strict=False)\n",
    "            \n",
    "    # creation of big bird\n",
    "    model = BigBird(\n",
    "        translator, discriminator, reconstructor , \n",
    "        vocab, gamma=0.99, clip_value=0.1, #for WGAN, useless if WGAN-GP is used \n",
    "        lr_G = 5e-5,\n",
    "        lr_D = 1e-4,\n",
    "        lr_R = 2e-5,\n",
    "        LAMBDA = 10, # Gradient penalty lambda hyperparameter\n",
    "        RL_scale = 1,\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_big_bird(vocab, N=4, d_model=128, d_ff=256, h=4, dropout=0.1, emb_share=True, bert_share=True)\n",
    "#model.load(\"Nest/NewbornBirdA2C_GumbelSoftmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_inv = {a:b for b, a in vocab.items()}\n",
    "def convert_ids_to_tokens(ids):\n",
    "    return [vocab_inv[i] for i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3830558f764a9095e0273f9a92514a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=17765), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin:\n",
      "['[CLS]', 'but', 'my', 'daughter', 'is', 'and', 'she', 'said', \"'\", 'well', 'it', \"'\", 's', 'not', 'great', ',', 'but', 'it', \"'\", 's', 'coffee', 'in', 'a', 'can', 'so', 'what', 'can', 'you', 'expect', \"'\", '.', 'i', 'took', 'a', 'sip', 'and', 'it', \"'\", 's', 'really', 'a', 'me', '##h', '!', 'product', '.', '.', '.', 'not', 'as', 'bad', 'as', 'it', 'could', 'be', 'but', 'also', 'not', 'as', 'good', 'as', 'it', 'could', 'be', 'either', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "summary:\n",
      "['[CLS]', 'dealings', '##ং', '##washed', 'debates', 'cougars', 'which', 'vast', 'guillermo', 'empathy', 'ـ', 'personality', 'executive', 'chen', '##erated', '##weather', '1900', 'cyril', 'ministry', 'fragrance']\n",
      "real summary:\n",
      "['[CLS]', 'i', 'am', 'not', 'a', 'coffee', 'drink', '##er', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "reconsturct out:\n",
      "['when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when']\n",
      "\n",
      "origin:\n",
      "['[CLS]', 'i', 'try', 'to', 'avoid', 'g', '##lu', '##ten', ',', 'so', 'cereal', '##s', 'have', 'pretty', 'much', 'become', 'a', 'thing', 'of', 'the', 'past', 'for', 'me', '.', 'i', 'was', 'very', 'excited', 'to', 'try', 'a', 'g', '##lu', '##ten', 'free', 'cereal', '.', 'if', 'you', 'only', 'care', 'about', 'the', 'taste', ',', 'then', 'this', 'cereal', 'is', 'fairly', 'enjoyable', '.', 'however', ',', 'it', 'is', 'advertised', 'as', 'a', 'health', 'food', 'and', 'this', 'is', 'where', 'i', 'run', 'into', 'issues', '.', 'the', 'first', 'ingredients', 'are', 'whole', 'grain', 'corn', 'meal', ',', 'eva', '##por', '##ated', 'cane', 'juice', ',', 'brown', 'rice', 'flour', 'and', 'in', '##ulin', '.', 'they', 'are', 'all', 'organic', ',', 'but', 'corn', 'is']\n",
      "summary:\n",
      "['[CLS]', '##aus', 'caribbean', 'songwriter', 'sudan', 'forrest', 'frankfurt', 'pacing', '[unused246]', 'rhythmic', 'implant', 'tasha', 'carnage', '##oons', 'careless', 'singh', 'strawberry', 'ways', 'fast', 'siberia']\n",
      "real summary:\n",
      "['[CLS]', 'great', 'idea', ',', 'not', 'so', 'great', 'execution', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "reconsturct out:\n",
      "['when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', '.', '.', '.', '.', '.', '.', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on']\n",
      "\n",
      "origin:\n",
      "['[CLS]', 'my', 'son', 'is', 'in', 'elementary', 'school', 'and', 'this', 'is', 'his', 'favorite', ',', 'favorite', 'preserves', 'for', 'his', 'peanut', 'butter', 'sandwich', '!', 'i', 'am', 'ordering', 'a', 'second', 'order', 'set', 'and', 'probably', '2', 'or', '3', 'extras', 'to', 'keep', 'him', 'happy', '!', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "summary:\n",
      "['[CLS]', 'involving', 'wilkins', 'avenue', 'scenic', 'famous', 'preschool', 'distinguish', 'luc', 'marian', 'euclidean', '##rso', '##grin', 'badge', 'unopposed', '##nesian', 'evangelical', 'christensen', '##〈', 'article']\n",
      "real summary:\n",
      "['[CLS]', 'my', 'son', 'loves', 'it', '!', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "reconsturct out:\n",
      "['when', 'when', 'when', 'when', 'i', 'i', 'i', '.', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and']\n",
      "\n",
      "origin:\n",
      "['[CLS]', 'i', 'just', 'tried', 'the', 'spicy', 'amy', \"'\", 's', 'chili', '.', 'it', 'was', 'the', 'best', 'ever', '.', 'i', 'will', 'try', 'the', 'other', 'products', 'as', 'well', '.', 'keep', 'up', 'the', 'good', 'work', '.', 'your', 'food', 'is', 'amazing', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "summary:\n",
      "['[CLS]', 'м', 'favourites', 'desires', 'these', 'supernatural', '##apa', '##´s', 'notebook', 'shan', '##bn', 'begs', 'transferring', 'sampled', 'excellent', 'ana', 'february', 'გ', 'compute', 'opposes']\n",
      "real summary:\n",
      "['[CLS]', 'yu', '##m', 'yu', '##m', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "reconsturct out:\n",
      "['i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i']\n",
      "\n",
      "origin:\n",
      "['[CLS]', 'i', 'have', 'struggled', 'for', 'the', 'last', 'year', 'with', 'my', '3', 'year', 'old', 'son', \"'\", 's', 'con', '##sti', '##pati', '##on', 'and', 'fear', 'of', 'going', '\"', 'po', '##o', '\"', 'on', 'the', 'pot', '##ty', '.', 'he', 'also', 'has', 'been', 'quite', 'hyper', '##active', 'in', 'the', 'past', 'year', ';', 'that', 'coincided', 'with', 'a', 'new', 'brother', '(', 'he', 'is', 'the', 'middle', 'child', ')', 'as', 'well', 'as', 'his', 'efforts', 'on', 'pot', '##ty', 'training', '.', 'at', 'the', 'suggestion', 'of', 'my', 'mother', 'in', 'law', ',', 'we', 'began', 'using', 'this', 'product', '(', '1', '/', '2', 'ts', '##p', '##n', 'per', 'day', ')', ',', 'and', '10', 'days', 'later', ',', 'he', 'began']\n",
      "summary:\n",
      "['[CLS]', 'wizard', 'documentaries', 'burlington', '##ister', 'threads', 'basis', '##evsky', '[unused234]', '##kk', '##aq', 'seizing', 'hills', 'cockpit', '##ress', 'mounting', 'locations', '##sons', 'suitcase', 'boundaries']\n",
      "real summary:\n",
      "['[CLS]', 'a', 'true', 'miracle', 'for', 'my', '3', 'year', 'old', 'son', '!', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "reconsturct out:\n",
      "['i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin:\n",
      "['[CLS]', 'these', 'chips', 'are', 'a', 'great', 'addition', 'to', 'lots', 'of', 'recipes', '!', 'i', 'add', 'them', 'to', 'pancakes', ',', 'banana', 'bread', ',', 'pumpkin', 'mu', '##ffin', '##s', 'or', 'bread', ',', 'my', 'morning', 'o', '##at', '##me', '##al', ',', 'o', '##at', '##me', '##al', 'cookie', 'dough', 'etc', '.', '.', '.', 'i', 'also', 'roll', 'a', 'tbs', '##p', '.', 'of', 'them', 'up', 'in', 'crescent', 'rolls', 'and', 'make', 'a', 'delicious', 'breakfast', 'treat', '!', 'very', 'versatile', 'and', 'flavor', '##ful', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "summary:\n",
      "['[CLS]', '区', 'curtiss', '##redo', 'chinese', 'inland', 'lisbon', 'returns', '##tan', 'swing', 'unaware', 'stephane', '##ulu', '##fles', 'attract', 'nostrils', '##ahl', 'winner', '##dity', 'ridge']\n",
      "real summary:\n",
      "['[CLS]', 'i', 'love', 'these', 'chips', '!', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "reconsturct out:\n",
      "['i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "\n",
      "origin:\n",
      "['[CLS]', 'we', 'love', 'the', 'smaller', 'sized', 'shredded', 'wheat', '.', 'other', 'brands', 'have', 'a', 'big', 'air', 'pocket', 'in', 'it', '.', 'my', 'kids', 'especially', 'the', 'little', '16', '##mont', '##h', 'old', 'i', 'baby', '##sit', 'loves', 'to', 'suck', 'all', 'the', 'milk', 'out', 'of', 'the', 'middle', 'of', 'the', 'cereal', 'pieces', 'and', 'then', 'chew', 'it', 'up', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'great', 'value', 'when', 'they', 'were', 'on', 'the', 'sub', '##scribe', 'and', 'save', '.', 'now', 'that', 'they', 'are', 'not', ',', 'i', 'may', 'have', 'to', 'find', 'it', 'elsewhere', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'we', 'have', 'compare', 'them', 'to', 'generic', 'target']\n",
      "summary:\n",
      "['[CLS]', 'emilia', 'fleeing', 'redistribution', 'straightened', 'п', 'newborn', 'onboard', '233', 'commissioning', 'bells', 'linden', 'regulates', 'drought', 'suzuki', 'hawker', 'resorts', 'runs', 'sharma', 'melvin']\n",
      "real summary:\n",
      "['[CLS]', 'yu', '##mmy', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "reconsturct out:\n",
      "['i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "\n",
      "origin:\n",
      "['[CLS]', 'i', 'have', 'a', 'very', 'pick', '##y', ',', 'nervous', 'cat', 'that', 'until', 'this', 'food', 'came', 'along', 'had', 'very', 'bad', 'problems', 'with', 'all', 'other', 'food', 'given', 'to', 'her', '.', 'she', 'had', 'a', 'tendency', 'to', 'get', 'sick', 'a', 'lot', 'and', 'groom', 'herself', 'bald', 'on', 'her', 'belly', 'and', 'legs', '.', 'she', 'also', 'had', 'problems', 'with', 'stomach', 'and', 'mouth', 'ul', '##cer', '##s', '.', 'all', 'of', 'that', 'has', 'gone', 'with', 'this', 'new', 'food', '.', 'she', 'has', 'also', 'grown', 'a', 'full', 'fur', 'coat', 'back', 'and', 'put', 'on', '2', '-', '3', 'pounds', '.', 'i', 'am', 'very', 'happy', 'and', 'impressed', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "summary:\n",
      "['[CLS]', 'polynomials', '##kle', 'gloucester', 'gates', 'camps', 'magnificent', 'swept', 'accent', 'noticing', 'master', '##dlow', 'puerto', '##oo', 'ω', 'nmi', 'next', 'vascular', 'theodor', 'leased']\n",
      "real summary:\n",
      "['[CLS]', 'miracle', 'cat', 'food', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "reconsturct out:\n",
      "['i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "\n",
      "origin:\n",
      "['[CLS]', 'i', \"'\", 've', 'tried', 'all', 'the', 'flavors', 'at', 'least', 'once', '.', 'original', 'and', 'ter', '##iya', '##ki', 'multiple', 'times', '.', 'first', 'batch', 'of', 'original', 'and', 'ter', '##iya', '##ki', 'was', 'great', ',', '2nd', 'and', '3rd', 'batch', '##es', 'too', 'over', '-', 'flavor', '##ed', '.', 'pepper', 'was', 'too', 'pepper', '##y', '(', 'and', 'i', 'love', 'pepper', ')', '.', 'hickory', 'was', 'too', 'strong', '/', 'salty', '.', 'just', 'not', 'consistent', '.', 'not', 'going', 'to', 'waste', 'more', 'money', 'on', 'this', 'product', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "summary:\n",
      "['[CLS]', 'stool', 'parking', 'not', 'defensive', '[unused918]', '##zy', 'brethren', '66', 'mori', 'command', 'l', '[unused680]', 'isolated', 'verbal', 'ম', 'cultivated', 'lawyers', '##anial', 'reviews']\n",
      "real summary:\n",
      "['[CLS]', 'inconsistent', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "reconsturct out:\n",
      "['i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "\n",
      "lay egg to ./Nest ... save as ./Nest/NewbornBirdA2C_LSTM_GumbelSoftmax\n",
      "origin:\n",
      "['[CLS]', 'i', 'like', '\"', 've', '##rno', '##r', \"'\", 's', '\"', ',', 'and', 'drink', 'it', 'all', 'the', 'time', '.', 'it', 'is', 'old', 'fashioned', 'drink', 'from', 'michigan', ',', 'since', '1866', '.', 'if', 'you', 'look', 'at', 'the', 'can', 'it', 'is', 'called', 'ginger', 'soda', 'instead', 'of', 'ale', 'and', 'is', 'different', 'than', 'typical', 'ginger', 'ale', 'in', 'taste', '.', 've', '##rno', '##rs', 'has', 'a', 'strong', 'vanilla', 'presence', ',', 'is', 'smooth', '##er', 'and', 'more', 'moist', ',', 'and', 'sweet', '##er', 'than', 'a', 'ginger', 'ale', '.', 'it', 'is', 'closer', 'to', 'a', 'cream', 'soda', 'than', 'ginger', 'ale', ',', 'very', 'good', '.', 'a', 'ginger', 'ale', 'is', 'more', 'dry', ',', 'higher', 'in']\n",
      "summary:\n",
      "['[CLS]', '[unused197]', 'internacional', 'girl', 'cho', 'courtship', 'captives', 'patrons', '##lled', '##⁶', 'our', 'argentina', '行', 'gym', '##nel', 'craters', '##lov', 'dvds', 'authentic', 'speed']\n",
      "real summary:\n",
      "['[CLS]', 'regional', 'ginger', 'soda', 'from', 'michigan', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "reconsturct out:\n",
      "['i', 'i', 'i', 'i', 'i', 'i', 'i', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin:\n",
      "['[CLS]', 'reality', 'strikes', '.', 'i', 'have', 'been', 'drinking', 'this', 'tea', 'every', 'night', 'for', '10', 'years', '!', 'it', 'is', 'the', 'best', 'thing', 'to', 'keep', 'me', 'on', 'the', 'straight', 'n', 'narrow', '!', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "summary:\n",
      "['[CLS]', 'challenges', '##year', 'can', '##g', 'percy', 'regards', 'after', '##ener', 'confederate', 'dairy', '##ص', 'department', '##mbling', 'tyler', 'tents', 'pile', 'thumped', 'menu', 'targets']\n",
      "real summary:\n",
      "['[CLS]', 'stay', 'regular', '!', '!', '!', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "reconsturct out:\n",
      "['i', 'i', 'i', 'i', 'i', 'i', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "\n",
      "origin:\n",
      "['[CLS]', 'i', 'try', 'to', 'find', 'healthy', 'or', '##hani', '##c', 'snack', 'for', 'my', 'kids', '.', 'especially', 'my', '1', '1', '/', '2', 'y', '##r', 'old', '.', 'while', 'brows', '##ing', 'the', 'organic', 'section', 'at', 'amazon', ',', 'i', 'came', 'across', 'these', '.', 'my', 'kids', 'love', 'these', '!', 'the', 'price', 'is', 'great', ',', 'i', 'signed', 'up', 'for', 'auto', 'delivery', '.', 'it', 'is', 'che', '##ep', '##er', 'and', 'you', 'can', 'cancel', 'when', 'ever', 'you', 'want', '.', 'i', 'get', 'the', 'strawberry', 'fruit', 'bars', 'as', 'well', '.', 'we', 'don', '##t', 'have', 'a', 'great', 'selection', 'of', 'earth', '##s', 'best', 'anywhere', 'around', ',', 'its', 'che', '##ep', '##er', 'on', 'amazon', '.', 'you']\n",
      "summary:\n",
      "['[CLS]', 'teeth', '264', 'nailed', 'mackenzie', 'to', '##zhou', \"'\", 'chant', 'some', 'fuji', 'works', '##0s', 'ro', 'copper', '朝', 'nirvana', 'purposely', 'airing', 'elites']\n",
      "real summary:\n",
      "['[CLS]', 'a', 'great', 'organic', 'snack', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "reconsturct out:\n",
      "['i', 'i', 'i', 'i', 'i', '/', '/', '/', '/', '/', '/', '/', '/', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "\n",
      "origin:\n",
      "['[CLS]', 'i', 'ordered', 'this', 'item', 'due', 'to', 'all', 'the', 'good', 'reviews', '.', 'we', 'tried', 'it', 'tonight', '.', 'both', 'the', 'husband', 'and', 'i', 'agreed', ':', 'edible', '.', 'i', 'was', 'very', 'disappointed', 'in', 'this', 'product', '.', 'but', 'then', 'we', 'never', 'eat', 'the', 'kraft', 'stuff', 'so', 'maybe', 'we', \"'\", 're', 'just', 'pick', '##y', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "summary:\n",
      "['[CLS]', 'suitable', 'the', '##ᵒ', '##lb', '##ℓ', 'subspecies', 'some', 'unpleasant', 'miocene', 'cabins', 'future', '郎', 'baldwin', 'months', 'arturo', 'nationality', '##schaft', 'elizabeth', 'diversion']\n",
      "real summary:\n",
      "['[CLS]', 'good', 'taste', '?', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "reconsturct out:\n",
      "['i', 'i', 'i', 'i', 'i', '/', '/', '/', '/', '/', '/', '/', '/', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "\n",
      "origin:\n",
      "['[CLS]', 'my', 'little', 'dog', 'enjoys', 'this', 'toy', 'quite', 'a', 'lot', '.', 'he', 'hasn', \"'\", 't', 'quite', 'figured', 'out', 'how', 'the', 'treats', 'come', 'out', 'yet', ',', 'but', 'he', \"'\", 's', 'always', 'happy', 'when', 'they', 'do', '!', 'lo', '##l', '!', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "summary:\n",
      "['[CLS]', 'crazy', 'limousine', '##mmy', 'з', 'lesson', '##akes', '##vana', 'be', '3', '##so', 'fe', 'cookie', 'unlike', 'right', 'disco', 'va', 'kat', 'in', 'while']\n",
      "real summary:\n",
      "['[CLS]', 'so', 'much', 'fun', '!', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "reconsturct out:\n",
      "['i', 'i', 'i', 'i', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = 1 if continue_from == None else (int(continue_from.split(\"Model\")[-1])+1)\n",
    "history = []\n",
    "\n",
    "\n",
    "#from tensorboardX import SummaryWriter\n",
    "#writer = SummaryWriter('mygraph')\n",
    "writer = None\n",
    "all_loss = []\n",
    "all_reward = []\n",
    "\n",
    "for epoch in range(start, num_epochs+1):\n",
    "    print(\"Epoch\", epoch)\n",
    "    \n",
    "    # training\n",
    "    stats = Stats()\n",
    "    model.train()\n",
    "    \n",
    "    rewards = []\n",
    "    \n",
    "    trange = tqdm(enumerate(data_gen_train()), total=total_train)\n",
    "    for i, batch in trange:\n",
    "        loss, score  = model.run_iter(batch.src, batch.src_mask, SUMM_MAX, batch.trg, writer, D_iters=5, verbose = 1)\n",
    "        trange.set_postfix(\n",
    "            **{'RL_loss': '{:.3f}'.format(loss[0])},\n",
    "            **{'G_loss': '{:.3f}'.format(loss[1])},\n",
    "            **{'D_loss': '{:.3f}'.format(loss[2])},\n",
    "            **{'real_score': '{:.3f}'.format(score[0])},\n",
    "            **{'fake_score': '{:.3f}'.format(score[1])},\n",
    "            **{'acc': '{:.3f}'.format(score[2])},\n",
    "            **{'reward':'{:.3f}'.format(score[3])}\n",
    "        )\n",
    "        stats.update(sum(loss), 1, log=0)\n",
    "        rewards.append(score[3])\n",
    "        \n",
    "    t_h = stats.history\n",
    "    history.append(t_h)\n",
    "    writer.add_scalar('reward', np.mean(t_h), epoch)\n",
    "    print(\"[info] epoch train loss:\", np.mean(t_h))\n",
    "    print(\"[info] epoch train reward:\", sum(rewards)/len(rewards))\n",
    "    all_loss.append(np.mean(t_h))\n",
    "    all_reward.append(sum(rewards)/len(rewards))\n",
    "writer.close()  \n",
    "#     try:\n",
    "#         torch.save({'model':model.state_dict(), 'training_history':t_h, 'validation_loss':np.mean(v_h)}, \n",
    "#                    \"trained/Model\"+str(epoch))\n",
    "#     except:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(all_reward)), all_reward)\n",
    "plt.plot(range(len(all_loss)), all_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model.all_rewards)\n",
    "#plt.plot(range(len(model.all_rewards)), model.all_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(range(len(model.all_rewards)-1), [sum(model.all_rewards[:i])/i for i in range(1,len(model.all_rewards))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
