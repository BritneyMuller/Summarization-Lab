{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_nb2 import *\n",
    "from dataset import make_data_generator\n",
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'data/'\n",
    "data_name = folder+'data.json'\n",
    "# validation_name = folder+'valid_seq.json'\n",
    "# testdata_name = folder+'testdata_seq.json'\n",
    "vocab_name = folder+'vocab.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "save_rate = 1 #how many epochs per modelsave\n",
    "#continue_from = \"trained/Model1\" # if none, put None\n",
    "continue_from = None\n",
    "epsilon = 1e-8\n",
    "validation_size = 10000\n",
    "device = torch.device('cuda')\n",
    "!mkdir -p trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = json.load(open(vocab_name, 'r'))\n",
    "VOC_SIZE = len(vocab)\n",
    "INPUT_MAX = 100\n",
    "SUMM_MAX = 20\n",
    "UNK = \"[UNK]\"\n",
    "BOS = \"[CLS]\"\n",
    "EOS = \"[SEP]\"\n",
    "PAD = \"[PAD]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading json\n",
      "load json done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00dd7f3f8e4495ea54229683f084933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=568454), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 90\n",
    "\n",
    "training_set, training_generator = make_data_generator(\\\n",
    "data_name, INPUT_MAX, SUMM_MAX, vocab[PAD], batch_size, cutoff=None, shuffle=True, num_workers=4)\n",
    "\n",
    "# validation_set, validation_generator = make_data_generator(\\\n",
    "# validation_name, INPUT_MAX, OUTPUT_MAX, vocab[PAD], batch_size, cutoff=validation_size, shuffle=False, num_workers=4)\n",
    "\n",
    "def data_gen_train():\n",
    "    for src, label, tgt in training_generator:\n",
    "        src = src.to(device)\n",
    "        #CrossEntropy 1~5 -> 0~4\n",
    "        label = (label - 1).long().to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        b = Batch(src, tgt, vocab[PAD])\n",
    "        b.label = label\n",
    "        yield b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "total_train = int(math.ceil(training_set.size / batch_size))\n",
    "# total_valid = int(math.ceil(validation_set.size / batch_size))\n",
    "# print(total_train, total_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_translator(src_vocab, tgt_vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1, emb_share=False):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    \n",
    "    src_emb = nn.Sequential(Embeddings(d_model, src_vocab), c(position))\n",
    "    tgt_emb = src_emb if emb_share else nn.Sequential(Embeddings(d_model, tgt_vocab), c(position))\n",
    "    \n",
    "    model = Translator(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "                             c(ff), dropout), N),\n",
    "        src_emb,\n",
    "        tgt_emb,\n",
    "        Generator(d_model, tgt_vocab))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classifier(src_vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    \n",
    "    bert = BERT(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        vocab[PAD]\n",
    "    )\n",
    "    \n",
    "    model = Classifier(\n",
    "        bert\n",
    "        # criterion = CE\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator(src_vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    \n",
    "    bert = BERT(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        vocab[PAD]\n",
    "    )\n",
    "    \n",
    "    model = Discriminator(\n",
    "        bert\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_param(model):\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "def make_big_bird(vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1, emb_share=False, bert_share=False):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    \n",
    "    vocab_sz = len(vocab)\n",
    "    \n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    \n",
    "    src_emb = nn.Sequential(Embeddings(d_model, vocab_sz), c(position))\n",
    "    if emb_share:        \n",
    "        tgt_emb = src_emb\n",
    "        bert_class_emb = src_emb\n",
    "        bert_discr_emb = src_emb\n",
    "    else:\n",
    "        tgt_emb = nn.Sequential(Embeddings(d_model, vocab_sz), c(position))\n",
    "        bert_class_emb = nn.Sequential(Embeddings(d_model, vocab_sz), c(position))\n",
    "        bert_discr_emb = nn.Sequential(Embeddings(d_model, vocab_sz), c(position))\n",
    "    \n",
    "    \n",
    "    bert_class = BERT(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        bert_class_emb,\n",
    "        vocab[PAD]\n",
    "    )\n",
    "    \n",
    "    if bert_share:\n",
    "        bert_discr = bert_class\n",
    "    else:\n",
    "        bert_discr = BERT(\n",
    "            Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "            bert_discr_emb,\n",
    "            vocab[PAD]\n",
    "        )\n",
    "    \n",
    "    translator = Translator(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "                             c(ff), dropout), N),\n",
    "        src_emb,\n",
    "        tgt_emb,\n",
    "        Generator(d_model, vocab_sz))\n",
    "    \n",
    "    classifier = Classifier(\n",
    "        bert_class\n",
    "        # criterion = BCE\n",
    "    )\n",
    "        \n",
    "    discriminator = Discriminator(\n",
    "        bert_discr\n",
    "    )\n",
    "        \n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for m in [translator, classifier, discriminator]:\n",
    "        init_param(m)\n",
    "            \n",
    "    # creation of big bird\n",
    "    model = BigBird(\n",
    "        translator, discriminator, classifier, \n",
    "        vocab, gamma=0.99, clip_value=0.1, #for WGAN, if WGAN-GP is used this is useless \n",
    "        lr_G = 1e-4,\n",
    "        lr_D = 5e-5,\n",
    "        lr_C = 5e-5,\n",
    "        LAMBDA = 10 # Gradient penalty lambda hyperparameter\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_big_bird(vocab, N=4, d_model=256, d_ff=1024, h=8, dropout=0.1, emb_share=True, bert_share=True)\n",
    "#model.load(\"Nest/NewbornBird\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_inv = {a:b for b, a in vocab.items()}\n",
    "def convert_ids_to_tokens(ids):\n",
    "    return [vocab_inv[i] for i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f862ed2306e5401d911cd71116adb3d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6317), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin:\n",
      "['[CLS]', 'ordered', 'this', 'on', 'sunday', 'and', 'rec', '##ie', '##ved', 'it', 'on', 'wednesday', '.', 'so', 'the', 'shipping', 'was', 'very', 'fast', '.', 'the', 'cups', 'aren', \"'\", 't', 'the', 'usual', 'cups', 'you', 'get', 'from', 'your', 'local', 'store', '.', 'looks', 'more', 'like', 'a', 'k', 'pouch', '.', 'that', 'being', 'said', 'it', 'is', 'good', 'coffee', '.', 'as', 'soon', 'as', 'i', 'got', 'them', 'i', 'had', 'to', 'try', 'it', 'out', '.', 'tasted', 'fresh', 'and', 'brew', '##ed', 'very', 'well', '.', 'i', 'am', 'glad', 'that', 'i', 'have', 'a', 'subscription', 'as', 'this', 'will', 'be', 'a', 'morning', 'regular', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "summary:\n",
      "['[CLS]', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle']\n",
      "real summary:\n",
      "['[CLS]', 'fast', 'and', 'good', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "sentiment: 3\n",
      "y: 4\n",
      "\n",
      "origin:\n",
      "['[CLS]', 'i', 'was', 'shocked', 'how', 'good', 'this', 'tea', 'is', '.', 'now', 'for', 'individuals', 'expecting', 'sweet', 'almost', 'thick', 'tea', ',', 'this', 'is', 'not', 'for', 'you', '.', 'the', 'sweetness', 'is', 'subtle', 'compared', 'to', 'homemade', 'sweet', 'tea', ',', 'but', 'it', 'is', 'definitely', 'good', 'enough', 'to', 'hit', 'the', 'spot', '.', 'plus', 'if', 'you', 'consider', 'the', 'cal', '##ori', '##e', 'count', 'of', 'only', '70', 'cal', '##ories', ',', 'i', 'think', 'it', 'is', 'worth', 'it', '.', 'i', 'will', 'be', 'purchasing', 'this', 'again', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "summary:\n",
      "['[CLS]', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle', '##ttle']\n",
      "real summary:\n",
      "['[CLS]', 'good', 'taste', 'for', 'a', 'quick', 'brew', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "sentiment: 3\n",
      "y: 4\n",
      "\n",
      "origin:\n",
      "['[CLS]', 'this', 'smoky', 'fa', '##l', '##vo', '##ured', 'black', 'tea', 'expert', '##ly', 'blend', '##s', 'the', 'essential', 'spices', 'of', 'ke', '##em', '##un', ',', 'dar', '##jee', '##ling', ',', 'ceylon', ',', 'and', 'assam', 'to', 'create', 'a', 'powerful', 'brew', '.', 'unlike', 'the', 'numerous', 'herbal', 'tea', '##s', 'that', 'i', 'frequently', 'ind', '##ul', '##ge', 'in', ',', 'the', 'flavour', 'of', 'this', 'brew', 'is', 'more', 'akin', 'to', 'the', 'traditional', 'tea', '##s', 'in', 'that', 'the', 'flavour', 'and', 'aroma', 'is', 'head', '##y', 'and', 'tends', 'more', 'toward', 'bitterness', '.', 'due', 'to', 'this', 'aspect', ',', 'i', 'found', 'that', 'cream', 'was', 'con', '##du', '##ci', '##ve', 'to', 'this', 'brew', 'and', 'that', 'for', 'those', 'who', 'prefer']\n",
      "summary:\n",
      "['[CLS]', '##it', '##it', '##it', '##it', '##it', '##it', '##it', '##it', '##it', '##it', '##it', '##it', '##it', '##it', '##it', '##it', '##it', '##it', '##it']\n",
      "real summary:\n",
      "['[CLS]', 'an', 'excellent', 'way', 'to', 'begin', 'the', 'morning', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "sentiment: 3\n",
      "y: 3\n",
      "\n",
      "origin:\n",
      "['[CLS]', 'i', 'decided', 'to', 'buy', 'these', 'sam', '##osa', '##s', 'and', 'try', 'them', 'out', '.', 'however', 'i', 'was', 'disappointed', 'that', 'they', 'lacked', 'some', 'essential', 'spices', 'and', 'tasted', 'bland', '.', 'in', 'addition', ',', 'i', 'paid', '$', '33', 'for', '6', 'boxes', ',', 'which', 'came', 'out', '$', '5', '.', '50', 'per', 'box', '.', 'today', ',', 'i', 'went', 'to', 'my', 'local', 'indian', 'grocery', 'store', ',', 'and', 'they', 'were', 'selling', 'it', 'for', '$', '2', '.', '99', 'per', 'box', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "summary:\n",
      "['[CLS]', 'floating', 'floating', 'floating', 'floating', 'floating', 'floating', 'floating', 'floating', 'floating', 'floating', 'floating', 'floating', 'floating', 'floating', 'floating', 'floating', 'floating', 'floating', 'floating']\n",
      "real summary:\n",
      "['[CLS]', 'expensive', 'and', 'bland', 'taste', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "sentiment: 4\n",
      "y: 1\n",
      "\n",
      "origin:\n",
      "['[CLS]', 'i', 'was', 'able', 'to', 'try', 'this', 'product', 'because', 'in', '##fl', '##uen', '##ster', 'sent', 'it', 'to', 'me', 'in', 'their', '#', 'mom', '##vo', '##x', '##box', 'for', 'testing', '.', 'honestly', 'i', 'wasn', \"'\", 't', 'able', 'to', 'try', 'it', 'personally', 'because', 'my', 'kids', 'snatched', 'it', 'from', 'me', 'ha', 'ha', '!', 'but', 'they', 'enjoyed', 'it', 'a', 'lot', '.', 'they', 'asked', 'me', 'if', 'i', 'would', 'buy', 'more', 'and', 'in', 'different', 'flavors', 'too', '.', 'good', 'and', 'heart', '##y', 'snack', 'for', 'kids', 'indeed', '!', 'thanks', '!', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "summary:\n",
      "['[CLS]', 'acted', 'acted', 'acted', 'acted', 'acted', 'acted', 'acted', 'acted', 'acted', 'acted', 'acted', 'acted', 'acted', 'acted', 'acted', 'acted', 'acted', 'acted', 'acted']\n",
      "real summary:\n",
      "['[CLS]', 'kids', 'loved', 'it', '!', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "sentiment: 4\n",
      "y: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = 1 if continue_from == None else (int(continue_from.split(\"Model\")[-1])+1)\n",
    "history = []\n",
    "for epoch in range(start, num_epochs+1):\n",
    "    print(\"Epoch\", epoch)\n",
    "    \n",
    "    # training\n",
    "    stats = Stats()\n",
    "    model.train()\n",
    "    trange = tqdm(enumerate(data_gen_train()), total=total_train)\n",
    "    for i, batch in trange:\n",
    "        \n",
    "        loss, score  = model.run_iter(batch.src, batch.src_mask, SUMM_MAX, batch.trg, batch.label, D_iters=2)\n",
    "        trange.set_postfix(\n",
    "            **{'RL_sample_loss': '{:.3f}'.format(loss[0])},\n",
    "            **{'RL_argmax_loss': '{:.3f}'.format(loss[1])},\n",
    "            **{'G_loss': '{:.3f}'.format(loss[2])},\n",
    "            **{'D_loss': '{:.3f}'.format(loss[3])},\n",
    "            **{'real_score': '{:.3f}'.format(score[0])},\n",
    "            **{'fake_score': '{:.3f}'.format(score[1])},\n",
    "            **{'sample_acc': '{:.3f}'.format(score[2])},\n",
    "            **{'argmax_acc': '{:.3f}'.format(score[3])}\n",
    "        )\n",
    "        stats.update(sum(loss), 1, log=0)\n",
    "        \n",
    "    t_h = stats.history\n",
    "    history.append(t_h)\n",
    "    \n",
    "    print(\"[info] epoch train loss:\", np.mean(t_h))\n",
    "    \n",
    "#     try:\n",
    "#         torch.save({'model':model.state_dict(), 'training_history':t_h, 'validation_loss':np.mean(v_h)}, \n",
    "#                    \"trained/Model\"+str(epoch))\n",
    "#     except:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
