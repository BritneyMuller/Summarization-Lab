{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_nb2 import *\n",
    "from dataset import make_data_generator\n",
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'data-giga/'\n",
    "data_name = folder+'train_seq.json'\n",
    "validation_name = folder+'valid_seq.json'\n",
    "testdata_name = folder+'testdata_seq.json'\n",
    "vocab_name = folder+'vocab.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "save_rate = 1 #how many epochs per modelsave\n",
    "continue_from = \"trained/Model1\" # if none, put None\n",
    "continue_from = None\n",
    "epsilon = 1e-8\n",
    "validation_size = 10000\n",
    "device = torch.device('cuda')\n",
    "!mkdir -p trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = json.load(open(vocab_name, 'r'))\n",
    "VOC_SIZE = len(vocab)\n",
    "INPUT_MAX = 50\n",
    "OUTPUT_MAX = 20\n",
    "UNK = \"[UNK]\"\n",
    "BOS = \"[CLS]\"\n",
    "EOS = \"[SEP]\"\n",
    "PAD = \"[PAD]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading json\n",
      "load json done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff5658549b24862b3d4a236475aeefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3796354), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loading json\n",
      "load json done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f46cc9edbb94e8aa8f2044232874881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7603), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "training_set, training_generator = make_data_generator(\\\n",
    "data_name, INPUT_MAX, OUTPUT_MAX, vocab[PAD], batch_size, cutoff=None, shuffle=True, num_workers=4)\n",
    "\n",
    "validation_set, validation_generator = make_data_generator(\\\n",
    "validation_name, INPUT_MAX, OUTPUT_MAX, vocab[PAD], batch_size, cutoff=validation_size, shuffle=False, num_workers=4)\n",
    "\n",
    "def data_gen_train():\n",
    "    for src,tgt in training_generator:\n",
    "#         src = Variable(src, requires_grad=False).to(device)\n",
    "#         tgt = Variable(tgt, requires_grad=False).to(device)\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        yield Batch(src, tgt, vocab[PAD])\n",
    "def data_gen_val():\n",
    "    for src,tgt in validation_generator:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        yield Batch(src, tgt, vocab[PAD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59319 119\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "total_train = int(math.ceil(training_set.size / batch_size))\n",
    "total_valid = int(math.ceil(validation_set.size / batch_size))\n",
    "print(total_train, total_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/george/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "model = make_model(VOC_SIZE, VOC_SIZE, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1, emb_share=True)\n",
    "\n",
    "if continue_from is not None:\n",
    "    saved_model = torch.load(continue_from)\n",
    "    model.load_state_dict(saved_model['model'])\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "# criterion = nn.NLLLoss(ignore_index=vocab[PAD], reduction='sum')\n",
    "criterion = LabelSmoothing(size=VOC_SIZE, padding_idx=vocab[PAD], smoothing=0.1)\n",
    "criterion.cuda()\n",
    "\n",
    "model_opt = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.998), eps=1e-9)\n",
    "\n",
    "loss_compute = SimpleLossCompute(model.generator, criterion, model_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_inv = {a:b for b, a in vocab.items()}\n",
    "def convert_ids_to_tokens(ids):\n",
    "    return [vocab_inv[i] for i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe18291d57fd4f05bb71dfd95c8c9d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59319), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1 Loss: 9.419542 Tokens / Sec: 690.337406\n",
      "\n",
      "['staples', 'staples', 'staples', 'staples', 'maglev', 'staples', 'bomb-making', 'russia-nato', 'bulent', 'staples', 'register', 'staples', 'bomb-making', 'bomb-making', 'fights', 'bomb-making', 'panicking', 'staples', 'staples']\n",
      "Step: 1001 Loss: 5.612229 Tokens / Sec: 2662.140242\n",
      "\n",
      "['[UNK]', \"'s\", 'in', 'in', 'in', '[UNK]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 2001 Loss: 5.389665 Tokens / Sec: 2721.051918\n",
      "\n",
      "['[UNK]', 'to', 'win', '[UNK]', '[SEP]', '[UNK]', '[SEP]', '[SEP]', 'world', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 3001 Loss: 4.681033 Tokens / Sec: 2504.729047\n",
      "\n",
      "['afghan', 'arabia', 'police', 'arrest', '##', '[SEP]', '[SEP]', 'afghanistan', 'of', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 4001 Loss: 4.713599 Tokens / Sec: 2550.088069\n",
      "\n",
      "['swiss', 'court', 'denies', 'trial', \"'s\", '[SEP]', '[SEP]', 'to', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 5001 Loss: 4.499488 Tokens / Sec: 2691.862636\n",
      "\n",
      "['brazil', \"'s\", 'president', 'president', 'to', 'to', 'new', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 6001 Loss: 4.167989 Tokens / Sec: 2511.118278\n",
      "\n",
      "['pakistani', 'police', 'in', 'killing', 'of', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 7001 Loss: 4.338363 Tokens / Sec: 2649.405022\n",
      "\n",
      "['[UNK]', \"'s\", 'the', '[UNK]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 8001 Loss: 4.078573 Tokens / Sec: 2555.719155\n",
      "\n",
      "['zambia', 'to', 'new', 'for', 'in', 'zambia', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 9001 Loss: 3.785793 Tokens / Sec: 2598.916242\n",
      "\n",
      "['myanmar', 'groups', 'urge', 'myanmar', 'to', 'against', 'stop', 'myanmar', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 10001 Loss: 3.598950 Tokens / Sec: 2691.174448\n",
      "\n",
      "['euro', 'stocks', 'in', 'dollar', 'in', 'tokyo', 'trade', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 11001 Loss: 3.865548 Tokens / Sec: 2594.823170\n",
      "\n",
      "['south', 'labor', 'says', 'zimbabwe', 'african', 'union', 'leader', '[SEP]', 'stay', 'off', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 12001 Loss: 3.437781 Tokens / Sec: 2540.449346\n",
      "\n",
      "['arafat', 'arrives', 'in', 'yemen', 'for', 'to', 'to', 'to', 'to', 'for', 'for', '[SEP]', 'for', '[SEP]', 'for', 'to', 'to', '[SEP]', '[SEP]']\n",
      "Step: 13001 Loss: 3.500929 Tokens / Sec: 2688.812585\n",
      "\n",
      "['renault', 'profit', '##', 'percent', 'rise', 'in', 'profit', 'quarter', 'profit', 'profit', '[SEP]', 'growth', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 14001 Loss: 3.550169 Tokens / Sec: 2645.510490\n",
      "\n",
      "['syrian', 'syrian', 'opposition', 'calls', 'call', 'for', 'for', 'to', 'syria', 'for', '[UNK]', 'of', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 15001 Loss: 3.388580 Tokens / Sec: 2541.779673\n",
      "\n",
      "['venus', 'williams', 'to', 'out', 'of', 'u.s.', 'cup', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 16001 Loss: 3.578870 Tokens / Sec: 2664.747664\n",
      "\n",
      "['republicans', 'to', 'ad', 'ad', 'on', 'kerry', 'tv', 'tv', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 17001 Loss: 3.459289 Tokens / Sec: 2642.364250\n",
      "\n",
      "['china', 'dissident', 'arrives', 'from', 'in', 'us', 'states', '[SEP]', 'in', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', 'in', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 18001 Loss: 3.302519 Tokens / Sec: 2538.168795\n",
      "\n",
      "['world', \"'s\", 'first', 'a###', 'a###', 'to', 'to', 'visit', 'in', 'beijing', '[SEP]', 'of', 'first', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 19001 Loss: 3.164388 Tokens / Sec: 2534.215951\n",
      "\n",
      "['##', 'killed', 'killed', 'in', 'car', 'plunges', 'into', 'canal', 'in', 'pakistan', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 20001 Loss: 3.313847 Tokens / Sec: 2547.767966\n",
      "\n",
      "['faa', 'to', 'build', '[UNK]', 'aircraft', '[SEP]', 'aircraft', 'aircraft', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 21001 Loss: 2.875894 Tokens / Sec: 2541.151886\n",
      "\n",
      "['earnhardt', \"'s\", 'after', '[UNK]', 'death', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 22001 Loss: 2.987422 Tokens / Sec: 2642.317846\n",
      "\n",
      "['sharks', '#', 'bruins', '#', '[SEP]', '#', '#', '#', '#', '#', '[SEP]', '#', '#', '#', '#', '#', '#', '#', '#']\n",
      "Step: 23001 Loss: 3.421658 Tokens / Sec: 2856.309350\n",
      "\n",
      "['falun', '##', 'falun', 'followers', 'at', 'held', 'held', 'in', 'hospital', 'hospital', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 24001 Loss: 3.227392 Tokens / Sec: 2540.840132\n",
      "\n",
      "['#', 'killed', 'in', 'clashes', 'wounded', 'rebels', 'clash', 'in', 'southern', 'philippines', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 25001 Loss: 2.939449 Tokens / Sec: 2713.574615\n",
      "\n",
      "['south', 'korean', 'tycoon', 'arrested', 'for', 'shooting', 'murder', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 26001 Loss: 2.987565 Tokens / Sec: 2623.090528\n",
      "\n",
      "['mugabe', 'president', 'mugabe', 'says', 'out', 'new', 'in', 'parliament', '[SEP]', 'new', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 27001 Loss: 3.083721 Tokens / Sec: 2568.026919\n",
      "\n",
      "['u.n.', 'council', 'votes', 'establish', 'up', 'of', 'saddam', 'regime', '[SEP]', '[SEP]', '[SEP]', 'of', 'of', '[SEP]', 'of', '[SEP]', '[SEP]', '[SEP]', 'of']\n",
      "Step: 28001 Loss: 2.921429 Tokens / Sec: 2805.761402\n",
      "\n",
      "['asean', 'countries', 'pledge', 'to', 'cooperation', 'in', 'asean', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', 'of', '[SEP]', '[SEP]']\n",
      "Step: 29001 Loss: 2.864733 Tokens / Sec: 2626.375471\n",
      "\n",
      "['astros', '#', 'brewers', '#', '[SEP]', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#']\n",
      "Step: 30001 Loss: 2.693151 Tokens / Sec: 2734.731681\n",
      "\n",
      "['google', 's', '$', '#.#', 'billion', 'bid', 'is', '$', '[SEP]', 'be', 'a', 'price', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 31001 Loss: 3.078602 Tokens / Sec: 2661.929022\n",
      "\n",
      "['zimbabwe', 'war', 'veterans', 'protest', 'court', 'court', '[SEP]', 'business', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 32001 Loss: 2.929423 Tokens / Sec: 2546.953167\n",
      "\n",
      "['cambodia', 'mark', 'independence', 'anniversary', 'of', 'independence', '[SEP]', 'poverty', '[SEP]', 'be', '[SEP]', '[SEP]', '[SEP]', 'of', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 33001 Loss: 2.968670 Tokens / Sec: 2516.418171\n",
      "\n",
      "['seattle', \"'s\", 'back', 'as', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 34001 Loss: 2.996581 Tokens / Sec: 2706.547542\n",
      "\n",
      "['sri', 'lankan', 'lankan', 'attacks', 'attacks', 'attack', 'presidential', 'candidate', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 35001 Loss: 3.156384 Tokens / Sec: 2775.925194\n",
      "\n",
      "['mugabe', 'mugabe', 'to', 'to', 'zimbabwe', '[SEP]', 'summit', 'summit', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 36001 Loss: 2.751179 Tokens / Sec: 2720.750145\n",
      "\n",
      "['czech', 'president', 'backs', 'turkish', 'eu', 'membership', '[SEP]', 'of', 'of', 'of', '[SEP]', 'of', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', 'calls']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 37001 Loss: 3.158911 Tokens / Sec: 2547.023294\n",
      "\n",
      "['government', 'muslim', 'of', 'the', '[SEP]', '[SEP]', 'the', 'muslim', 'state', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 38001 Loss: 3.006186 Tokens / Sec: 2686.488248\n",
      "\n",
      "['mexican', 'mexican', 'court', 'launches', 'probe', 'deadly', 'blaze', 'that', 'killing', '[SEP]', '[SEP]', 'blaze', 'deaths', 'blaze', 'school', 'blaze', '[SEP]', '[SEP]', 'killing']\n",
      "Step: 39001 Loss: 2.768993 Tokens / Sec: 2572.578115\n",
      "\n",
      "['abacha', 'dictator', 'abacha', 'at', 'rights', 'to', '[SEP]', '[SEP]', 'as', 'of', 'of', '[SEP]', 'of', 'of', 'of', 'of', 'abacha', 'of', 'of']\n",
      "Step: 40001 Loss: 2.867320 Tokens / Sec: 2681.376820\n",
      "\n",
      "['germany', 'posts', 'posts', 'first', 'net', '[SEP]', 'of', 'lower', 'percent', '[SEP]', 'lower', '[SEP]', 'lower', 'lower', 'lower', 'lower', '[SEP]', '[SEP]', 'lower']\n",
      "Step: 41001 Loss: 2.878978 Tokens / Sec: 2587.217863\n",
      "\n",
      "['manhattan', 'club', 'to', 'new', 'of', 'it', 'will', 'change', 'gay', 'gay', '[SEP]', '[SEP]', '[SEP]', 'gay', 'gay', 'gay', 'gay', 'gay', '[SEP]']\n",
      "Step: 42001 Loss: 2.845684 Tokens / Sec: 2637.375972\n",
      "\n",
      "['europe', 'finds', 'europe', \"'s\", 'carbon', 'role', 'in', 'arctic', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 43001 Loss: 2.892982 Tokens / Sec: 2646.190147\n",
      "\n",
      "['china', 'reduces', 'treasury', 'of', 'treasury', 'debt', '[SEP]', 'june', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', 'up', 'rises', '[SEP]', '[SEP]', 'up', '[SEP]']\n",
      "Step: 44001 Loss: 2.719419 Tokens / Sec: 2489.111818\n",
      "\n",
      "['new', 'teacher', 'in', 'a', 'teacher', 'in', 'african', 'african', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', 'of', '[SEP]', 'of', '[SEP]']\n",
      "Step: 45001 Loss: 2.779430 Tokens / Sec: 2436.295859\n",
      "\n",
      "['u.n.', 'concerned', 'about', 'economic', 'of', '[UNK]', 'measures', 'measures', '[SEP]', '[SEP]', '[SEP]', '[SEP]', 'of', '[SEP]', '[SEP]', '[SEP]', 'developing', '[SEP]', '[SEP]']\n",
      "Step: 46001 Loss: 2.412106 Tokens / Sec: 2553.229847\n",
      "\n",
      "['rumsfeld', 'says', 'us', 'strikes', 'top', 'iraqi', 'leadership', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 47001 Loss: 2.564692 Tokens / Sec: 2618.682050\n",
      "\n",
      "['iraq', 'says', 'for', 'boycott', 'countries', 'to', 'boycott', 'kurdish', 'in', 'iraq', 'affairs', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 48001 Loss: 2.807616 Tokens / Sec: 2560.328706\n",
      "\n",
      "['itf', 'says', 'out', 'on', 'blood', 'blood', 'tests', 'for', 'epo', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', 'says', '[SEP]', '[SEP]', 'says', 'says']\n",
      "Step: 49001 Loss: 2.551250 Tokens / Sec: 2642.949938\n",
      "\n",
      "['explosion', 'explosion', 'kills', 'kazakhstan', 'kills', 'at', 'miners', '##', 'miners', '[SEP]', 'missing', 'missing', '[SEP]', 'missing', '[SEP]', '[SEP]', 'reported', 'of', 'of']\n",
      "Step: 50001 Loss: 2.894261 Tokens / Sec: 2703.904674\n",
      "\n",
      "['wto', 'chief', 'lamy', 'as', 'candidate', 'candidate', 'for', 'succeed', 'the', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 51001 Loss: 2.677828 Tokens / Sec: 2482.314909\n",
      "\n",
      "['fda', 'fails', 'fda', 'warning', 'on', '[SEP]', '[UNK]', '[SEP]', 'of', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 52001 Loss: 2.797096 Tokens / Sec: 2578.649566\n",
      "\n",
      "['north', 'korea', 'air', 'force', 'conducts', 'in', 'south', 'level', 'since', '####', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 53001 Loss: 2.677039 Tokens / Sec: 2668.784412\n",
      "\n",
      "['paris', 'to', 'olympic', 'to', 'plans', 'for', 'swimming', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', 'events', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 54001 Loss: 2.807716 Tokens / Sec: 2679.427950\n",
      "\n",
      "['lawmakers', 'lawmakers', 'nears', 'legislature', 'signs', 'begin', 'results', 'a', 'a', 'sign', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 55001 Loss: 2.437956 Tokens / Sec: 2593.628635\n",
      "\n",
      "['yosemite', 'rain', 'expected', 'yosemite', 'national', 'park', '[SEP]', 'homes', 'homes', '[SEP]', 'evacuation', 'of', 'flee', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 56001 Loss: 2.391517 Tokens / Sec: 2651.034939\n",
      "\n",
      "['yankees', 'lose', 'to', 'hit', 'in', 'into', '####', '[SEP]', '#', 'to', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 57001 Loss: 2.588899 Tokens / Sec: 2488.959380\n",
      "\n",
      "['switzerland', 'authorities', 'investigate', 'two', 'people', 'suspected', 'deadly', 'attacks', 'attacks', '[SEP]', 'out', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 58001 Loss: 2.632089 Tokens / Sec: 2642.088206\n",
      "\n",
      "['google', 'launches', 'launches', 'search', 'search', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 59001 Loss: 2.528479 Tokens / Sec: 2700.242026\n",
      "\n",
      "['sierra', 'sierra', 'leone', 'president', 'to', 'to', 'join', 'environmental', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 59319 Loss: 2.881538 Tokens / Sec: 226.0999072\n",
      "[info] epoch train loss: 3.2433314303208407.012671\n",
      "[info] epoch valid loss: 2.5338707691481135\n",
      "Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf3964485d4d41eebf697aa67de717b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59319), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1 Loss: 2.553679 Tokens / Sec: 1128.361700\n",
      "\n",
      "['turkey', 'to', 'parliamentary', 'commission', 'to', 'of', 'former', '[SEP]', 'corruption', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 1001 Loss: 2.675547 Tokens / Sec: 2618.954418\n",
      "\n",
      "['us', 'president', 'candidates', 'face', 'tough', 'us', '[SEP]', 'us', '[SEP]', 'musharraf', 'seek', 'on', 'support', 'musharraf', 's', 'suspension', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 2001 Loss: 2.509633 Tokens / Sec: 2552.862193\n",
      "\n",
      "['six', 'portuguese', 'rugby', 'jailed', '##', 'in', 'jail', '[SEP]', 'world', 'world', 'cup', 'qualifying', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', 'qualifying', '[SEP]']\n",
      "Step: 3001 Loss: 2.423761 Tokens / Sec: 2682.436662\n",
      "\n",
      "['israeli', 'protest', 'peres', 'of', 'peres', \"'s\", 'peres', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 4001 Loss: 2.569580 Tokens / Sec: 2588.940649\n",
      "\n",
      "['israeli', 'killed', 'militants', 'killed', 'killed', 'killed', 'in', 'gaza', 'shootout', '[SEP]', 'israeli', '[SEP]', 'contributed', 'contributed', 'contributed', 'killed', 'israeli', 'contributed', 'contributed']\n",
      "Step: 5001 Loss: 2.521199 Tokens / Sec: 2610.296935\n",
      "\n",
      "['indonesia', 'percent', 'of', 'illegal', 'reef', 'illegal', 'indonesia', \"'s\", 'east', 'kalimantan', '[SEP]', '[SEP]', '[SEP]', 'of', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 6001 Loss: 2.740215 Tokens / Sec: 2452.817124\n",
      "\n",
      "['khmer', 'rebels', 'sentenced', 'for', '##', 'mine', 'murder', '[SEP]', 'murder', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 7001 Loss: 2.765284 Tokens / Sec: 2502.926223\n",
      "\n",
      "['[UNK]', 'rally', 'protest', 'and', '[UNK]', 'for', 'rights', 'rights', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 7913 Loss: 2.678480 Tokens / Sec: 2531.092520\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 11001 Loss: 2.700348 Tokens / Sec: 2610.895362\n",
      "\n",
      "['cuban', 'cuban', 'political', 'dies', 'in', 'hunger', 'strike', '[SEP]', 'out', '[SEP]', 'out', '[SEP]', '[SEP]', 'ill', '[SEP]', 'ill', '[SEP]', '##', '[SEP]']\n",
      "Step: 11900 Loss: 2.523179 Tokens / Sec: 2698.194795\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 15001 Loss: 2.454744 Tokens / Sec: 2648.830564\n",
      "\n",
      "['us', 'tv', 'news', 'legend', 'walter', 'walter', 'dies', 'at', '##', '##', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 15875 Loss: 2.280192 Tokens / Sec: 2703.806466\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 19001 Loss: 2.466532 Tokens / Sec: 2525.681932\n",
      "\n",
      "['denktash', 'says', 'cyprus', 'referendum', 'as', 'disgrace', '[SEP]', 'un', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 19823 Loss: 2.472369 Tokens / Sec: 2620.178596\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 23001 Loss: 2.279946 Tokens / Sec: 2602.463702\n",
      "\n",
      "['new', 'ferry', 'to', 'in', 'tonga', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 23754 Loss: 2.692589 Tokens / Sec: 2651.450500\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 27001 Loss: 2.335746 Tokens / Sec: 2514.340667\n",
      "\n",
      "['[UNK]', \"'s\", 'sports', 'count', 'to', 'determine', 'whether', 'winner', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 27832 Loss: 2.680645 Tokens / Sec: 2498.059685\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 31001 Loss: 2.404490 Tokens / Sec: 2508.589424\n",
      "\n",
      "['bangladesh', 'bangladesh', 'braces', 'for', 'massive', '[SEP]', 'after', 'storm', '[SEP]', '[SEP]', 'for', 'for', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', 'for']\n",
      "Step: 31884 Loss: 2.346927 Tokens / Sec: 2571.955770\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 35001 Loss: 2.463409 Tokens / Sec: 2631.754674\n",
      "\n",
      "['u.s.', 'deputy', 'secretary', 'of', 'state', 'to', 'visit', 'turkey', '[SEP]', 'week', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '#', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 35937 Loss: 2.273108 Tokens / Sec: 2702.563756\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 39001 Loss: 2.661212 Tokens / Sec: 2628.379671\n",
      "\n",
      "['julius', 'company', 'of', 'legal', 'to', 'wikileaks', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[UNK]']\n",
      "Step: 39960 Loss: 2.282468 Tokens / Sec: 2495.741851\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 41001 Loss: 2.425996 Tokens / Sec: 2538.074071\n",
      "\n",
      "['police', 'arrests', 'arrest', '##', 'members', '[SEP]', 'suspected', 'for', 'for', '[SEP]', '[SEP]', '[SEP]', 'members', 'suspects', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 42001 Loss: 2.392684 Tokens / Sec: 2625.713968\n",
      "\n",
      "['bangladesh', 'to', 'set', 'industrial', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', 'industries', '[SEP]']\n",
      "Step: 43001 Loss: 2.283810 Tokens / Sec: 2588.340169\n",
      "\n",
      "['al-zarqawi', 'u.s.', 'for', 'with', 'al-zarqawi', 'in', '[SEP]', '[SEP]', '[SEP]', 'with', 'with', 'with', '[SEP]', '[SEP]', \"'s\", '[SEP]', '[SEP]', 'with', 'with']\n",
      "Step: 44001 Loss: 2.359656 Tokens / Sec: 2570.169101\n",
      "\n",
      "['somali', 'pirates', 'hold', 'ukrainian', 'demand', '[SEP]', '##', 'ship', 'ship', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 45001 Loss: 2.296386 Tokens / Sec: 2554.039582\n",
      "\n",
      "['u.n.', 'to', 'to', 'end', '[UNK]', 'of', 'tainted', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 46001 Loss: 2.320136 Tokens / Sec: 2576.209703\n",
      "\n",
      "['ukrainian', 'arrested', 'arrested', 'be', 'extradited', 'from', 'greece', '[SEP]', '[SEP]', 'missing', 'of', 'missing', 'ship', 'ship', 'ship', 'missing', 'missing', 'ship', 'ship']\n",
      "Step: 47001 Loss: 2.493698 Tokens / Sec: 2535.051175\n",
      "\n",
      "['clinton', 'leaders', 'is', 'politics', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 48001 Loss: 2.540390 Tokens / Sec: 2584.906108\n",
      "\n",
      "['report', 'needs', 'may', 'call', 'for', 'u.s.', 'us', '[SEP]', '[SEP]', 'report', 'report', 'report', 'report', 'report', 'report', 'report', 'report', 'report', 'report']\n",
      "Step: 49001 Loss: 2.664037 Tokens / Sec: 2722.038514\n",
      "\n",
      "['qwest', 'phone', 'expects', 'to', 'restate', 'earnings', '[SEP]', 'to', 'higher', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "Step: 49336 Loss: 2.409628 Tokens / Sec: 2697.828033\r"
     ]
    }
   ],
   "source": [
    "start = 1 if continue_from == None else (int(continue_from.split(\"Model\")[-1])+1)\n",
    "for epoch in range(start, num_epochs+1):\n",
    "    print(\"Epoch\", epoch)\n",
    "    \n",
    "    # training\n",
    "    stats = Stats()\n",
    "    model.train()\n",
    "    for i, batch in enumerate(tqdm(data_gen_train(), total=total_train)):\n",
    "        out = model.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        stats.update(loss, batch.ntokens, log=1)\n",
    "        \n",
    "        if( i % 1000 == 0):\n",
    "            probs = model.generator(out) \n",
    "            print(\"\\n\")\n",
    "            next_words = torch.argmax(probs, dim=-1, keepdim=True)            \n",
    "            print(convert_ids_to_tokens([i.item() for i in next_words[0]]))\n",
    "            \n",
    "    t_h = stats.history\n",
    "    \n",
    "    # validation\n",
    "    stats = Stats()\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(data_gen_val()):\n",
    "        out = model.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        stats.update(loss, batch.ntokens, log=1)\n",
    "    v_h = stats.history\n",
    "    \n",
    "    print(\"[info] epoch train loss:\", np.mean(t_h))\n",
    "    print(\"[info] epoch valid loss:\", np.mean(v_h))\n",
    "    \n",
    "    try:\n",
    "        torch.save({'model':model.state_dict(), 'training_history':t_h, 'validation_loss':np.mean(v_h)}, \n",
    "                   \"trained/Model\"+str(epoch))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode_batch(model, src, src_mask, max_len, start_symbol):\n",
    "    batch_size = src.shape[0]\n",
    "    \n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(batch_size, 1).fill_(start_symbol).type_as(src.data)\n",
    "    \n",
    "    modelouts = None\n",
    "    \n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src_mask))\n",
    "        #out = model.decode(memory, src_mask, ys, future_mask(ys.size(1)).type_as(src_mask))\n",
    "        #print(out.shape) #bs,len,256\n",
    "        probs = model.generator(out[:, -1, :])\n",
    "        #print(probs.shape) #bs,voc\n",
    "        \n",
    "        modelouts = out\n",
    "        \n",
    "        next_words = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "        \n",
    "        #print(next_words.shape)        \n",
    "        #print(ys.shape) #both bs,1\n",
    "        \n",
    "#         print(next_words)\n",
    "        \n",
    "        ys = torch.cat((ys, next_words), dim=1)\n",
    "    return ys, modelouts\n",
    "\n",
    "vocab_inv = {a:b for b, a in vocab.items()}\n",
    "def convert_ids_to_tokens(ids):\n",
    "    return [vocab_inv[i] for i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_compute2 = SimpleLossCompute2(model.generator, criterion)\n",
    "model.eval()\n",
    "for batch in data_gen_train():    \n",
    "    srcs = batch.src\n",
    "    src_masks = batch.src_mask.byte()\n",
    "    \n",
    "    trgs = batch.trg\n",
    "    trg_masks = batch.trg_mask\n",
    "        \n",
    "    bs = srcs.shape[0]\n",
    "    \n",
    "    outs, modelouts = greedy_decode_batch(model, srcs, src_masks, max_len=OUTPUT_MAX, start_symbol=vocab[BOS])\n",
    "    \n",
    "    loss = loss_compute(modelouts, batch.trg_y, batch.ntokens)\n",
    "    \n",
    "    print(loss / batch.ntokens)\n",
    "    \n",
    "    for j, (out_tensor, tgt_tensor) in enumerate(zip(outs, trgs)):        \n",
    "        tokens = convert_ids_to_tokens(out_tensor.cpu().numpy())  \n",
    "        print(tokens)\n",
    "              \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab[PAD], vocab[EOS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
