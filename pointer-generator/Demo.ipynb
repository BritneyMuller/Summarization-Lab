{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "from nltk import word_tokenize as tokenize\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "\n",
    "vocab_name = \"preprocessing-cnn-all/vocab.json\"\n",
    "num_threads = 6\n",
    "vocab = json.load(open(vocab_name, 'r'))\n",
    "vocab_inv = {ind:word for word, ind in vocab.items()}\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 256\n",
    "EMB_DIM = 50\n",
    "INPUT_MAX = 2000\n",
    "OUTPUT_MAX = 100\n",
    "num_epochs = 20\n",
    "save_rate = 4 #how many epochs per modelsave\n",
    "clip_grad_norm = 15. #maximum gradient norm\n",
    "continue_from = \"models/Model3\" # if none, put None\n",
    "# continue_from = None\n",
    "epsilon = 1e-10\n",
    "VOC_SIZE = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference(nn.Module):\n",
    "    def __init__(self, hidden_dim, emb_dim, input_len, output_len, voc_size):\n",
    "        super(Inference, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.voc_size = voc_size\n",
    "    \n",
    "        self.emb_layer = nn.Embedding(voc_size, emb_dim)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.decoder = nn.LSTM(emb_dim, hidden_dim*2, num_layers=1, batch_first=True)\n",
    "        \n",
    "        self.attention_softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.pro_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*4, voc_size, bias=True),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.pgen_layer = nn.Sequential(\n",
    "            nn.Linear(4*hidden_dim+emb_dim, 1, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size =x.shape[0]\n",
    "        # encoder\n",
    "        x_emb = self.emb_layer(x)\n",
    "#         print(x_emb.shape)\n",
    "        x_bilstm,(h, c) = self.encoder(x_emb)\n",
    "#         print(x_bilstm.shape, h.shape, c.shape)\n",
    "#         h = h.transpose(0, 1).contiguous().view(1, batch_size, h.shape[-1]*2)\n",
    "#         c = c.transpose(0, 1).contiguous().view(1, batch_size, c.shape[-1]*2)\n",
    "        h = h.transpose(0, 1).contiguous()\n",
    "        c = c.transpose(0, 1).contiguous()\n",
    "        h = h.view(batch_size, 1, h.shape[-1]*2)\n",
    "        c = c.view(batch_size, 1, c.shape[-1]*2)\n",
    "        h = h.transpose(0, 1).contiguous()\n",
    "        c = c.transpose(0, 1).contiguous()\n",
    "#         print(x_bilstm.shape, h.shape, c.shape)\n",
    "        \n",
    "\n",
    "        \n",
    "        ## decoder\n",
    "#         ans_emb = self.emb_layer(ans)\n",
    "#         print(ans_emb.shape)\n",
    "        out_h, out_c = (h, c)\n",
    "        first = True\n",
    "        \n",
    "        # batch, 1, emb; content: vocab['<bos>']\n",
    "        t_bos = torch.tensor([vocab['<bos>']]).to(device)\n",
    "#         print('bos', t_bos.shape)\n",
    "        t_bos = t_bos.repeat(batch_size, 1)\n",
    "#         print('rbos', t_bos.shape)\n",
    "        decoder_input_emb = self.emb_layer(t_bos)\n",
    "#         print('bosemb', decoder_input_emb.shape)\n",
    "#         print(self.output_len)\n",
    "        for i in range(self.output_len):\n",
    "            w = decoder_input_emb\n",
    "#             print('w', w.shape)\n",
    "            out, (out_h, out_c) = self.decoder(w, (out_h, out_c))\n",
    "#             print('out', out.shape)\n",
    "            attention = torch.bmm(x_bilstm, out.transpose(1, 2)).view(batch_size,x_bilstm.shape[1])\n",
    "            attention = self.attention_softmax(attention)\n",
    "\n",
    "            pointer_prob = torch.zeros([batch_size, self.voc_size], dtype=torch.float).to(device)\n",
    "            pointer_prob = pointer_prob.scatter_add_(dim=1, index=x, src=attention).view(batch_size, 1, self.voc_size)\n",
    "            \n",
    "            context_vector = torch.bmm(attention.view(batch_size, 1, self.input_len), x_bilstm)\n",
    "\n",
    "            \n",
    "            feature = torch.cat((out, context_vector), -1)\n",
    "            \n",
    "            pgen_feat = torch.cat((context_vector, out, w), -1)\n",
    "\n",
    "            distri = self.pro_layer(feature)\n",
    "            pgen = self.pgen_layer(pgen_feat)\n",
    "            final_dis = pgen*distri + (1.-pgen)*pointer_prob + epsilon\n",
    "            \n",
    "            ans_indices = torch.argmax(final_dis, dim=-1, keepdim=False)\n",
    "            decoder_input_emb = self.emb_layer(ans_indices)\n",
    "            \n",
    "            ######### decoder attention\n",
    "            if first:\n",
    "                decoder_attn = attention.view(batch_size, 1, attention.shape[1])\n",
    "                #print(attention.shape) torch.Size([1, 50])\n",
    "            else:\n",
    "                decoder_attn = torch.cat((decoder_attn, attention.view(batch_size, 1, attention.shape[1])), 1)\n",
    "            ######### end \n",
    "            \n",
    "            if first:\n",
    "                first = False\n",
    "                ans_seq = ans_indices\n",
    "            else:\n",
    "                ans_seq = torch.cat((ans_seq, ans_indices), 1)\n",
    "                       \n",
    "        \n",
    "        return ans_seq, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = torch.load(continue_from)\n",
    "\n",
    "inf_model = Inference(HIDDEN_DIM, EMB_DIM, INPUT_MAX, OUTPUT_MAX, VOC_SIZE).to(device)\n",
    "inf_model.load_state_dict(saved_model['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s):\n",
    "    s = s.lower()\n",
    "    words = ['<bos>'] + tokenize(s) + ['<eos>']\n",
    "    seq = []\n",
    "    for w in words:        \n",
    "        try:\n",
    "            wid = vocab[w]\n",
    "        except KeyError:\n",
    "            wid = vocab[\"<unk>\"]\n",
    "        seq.append(wid)\n",
    "    \n",
    "    pad_len = INPUT_MAX - len(seq)\n",
    "    seq = [vocab['<pad>']]*pad_len + seq\n",
    "#     return torch.tensor([seq[-INPUT_MAX:]])\n",
    "    return torch.tensor([seq[:INPUT_MAX]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readable(sent):\n",
    "    try:\n",
    "        end = sent.index('<eos>')\n",
    "    except ValueError:\n",
    "        end = len(sent)\n",
    "    sent = \" \".join(sent[:end])\n",
    "    sent = sent.replace(\"<bos>\", '').replace(\"<eos>\", '').replace(\"<unk>\", '-UNK-').replace(\"<pad>\", '')\n",
    "    sent = sent.capitalize()\n",
    "    return \" \".join(sent.split())\n",
    "def tensor2sent(t):\n",
    "    return [vocab_inv[wid.item()] for wid in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def print_attn(a, doc, summ):   \n",
    "    \n",
    "    array = a.cpu().numpy()\n",
    "    sc = 0.6\n",
    "    fig, ax = plt.subplots(figsize=(30*sc, 100*sc))\n",
    "    im = ax.imshow(array, cmap='hot')\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(len(doc)))\n",
    "    ax.set_yticks(np.arange(len(summ)))\n",
    "    # ... and label them with the respective list entries\n",
    "    ax.set_xticklabels(doc, fontsize=28*sc)#12\n",
    "    ax.set_yticklabels(summ, fontsize=28*sc)#12\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    ax.set_title(\"Attention matrix\", fontsize=20)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document to summarize: on wednesday , united states president donald trump signed an executive order escalating his administration 's campaign against chinese telecoms giant huawei , raising pressure on allies to follow suit in banning the company from their 5g and other networks . the us claims huawei , one of china 's most important companies , poses a spying risk to western technology infrastructure . the latest move against the firm comes amid a worsening trade war between beijing and washington , after talks expected to bring a breakthrough fell apart , resulting in billions of dollars in further tariffs from both sides . while some us allies -- notably australia and new zealand -- have followed trump 's lead on huawei , others have been more reticent . europe in particular is split over whether to ban the company , a market leader on 5g technology which is expected to be the lifeblood of the new economy . the huawei issue cuts to the heart of tensions between security and economic interests when it comes to china and chinese influence . while many countries around the world share washington 's suspicion -- even hostility -- towards beijing , they are unwilling to take the economic hit that openly standing apart from china would entail .\n",
      "\n",
      "[input] on wednesday , united states president donald trump signed an executive order escalating his administration 's campaign against chinese telecoms giant huawei , raising pressure on allies to follow suit in banning the company from their 5g and other networks . the us claims huawei , one of china 's most important companies , poses a spying risk to western technology infrastructure . the latest move against the firm comes amid a worsening trade war between beijing and washington , after talks expected to bring a breakthrough fell apart , resulting in billions of dollars in further tariffs from both sides . while some us allies -- notably australia and new zealand -- have followed trump 's lead on huawei , others have been more reticent . europe in particular is split over whether to ban the company , a market leader on 5g technology which is expected to be the lifeblood of the new economy . the huawei issue cuts to the heart of tensions between security and economic interests when it comes to china and chinese influence . while many countries around the world share washington 's suspicion -- even hostility -- towards beijing , they are unwilling to take the economic hit that openly standing apart from china would entail .\n",
      "\n",
      "[output]\n",
      " Donald trump says huawei is a spying risk to western technology infrastructure . he says the us claims huawei , one of china 's most important companies , poses a spying risk . he says the company is expected to be the lifeblood of the new economy .\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():    \n",
    "    while True:\n",
    "        rawdoc = input(\"Document to summarize: \")\n",
    "        wordseq = preprocess(rawdoc)\n",
    "        wordseq = wordseq.to(device)\n",
    "\n",
    "        summ_sent = tensor2sent(wordseq.view(INPUT_MAX))\n",
    "        print('\\n[input]', readable(summ_sent))\n",
    "\n",
    "        predict, attn = inf_model.forward(wordseq)\n",
    "        predict = predict.view(OUTPUT_MAX)\n",
    "#         print(attn[0].shape)\n",
    "\n",
    "\n",
    "\n",
    "        sent = tensor2sent(predict)\n",
    "        print('\\n[output]\\n', readable(sent))\n",
    "#         print_attn(attn[0], summ_sent, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
